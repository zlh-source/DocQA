IN-CONTEXT LEARNING FOR FEW-SHOT NESTED NAMED ENTITY RECOGNITIONMeishan Zhang1, Bin Wang1, Hao Fei2∗, Min Zhang11Harbin Institute of Technology (Shenzhen), Shenzhen, China2National University of Singapore, SingaporeABSTRACTIn nested Named entity recognition (NER), entities are nestedwith each other, and thus requiring more data annotations toaddress. This leads to the development of few-shot nestedNER, where the prevalence of pretrained language modelswith in-context learning (ICL) offers promising solutions.In this work, we introduce an effective and innovative ICLframework for the setting of few-shot nested NER. We im-prove the ICL prompt by devising a novel example demon-stration selection mechanism, EnDe retriever. In EnDe re-triever, we employ contrastive learning to perform three typesof representation learning, in terms of semantic similarity,boundary similarity, and label similarity, to generate high-quality demonstration examples. Extensive experiments overthree nested NER and four flat NER datasets demonstrate theefficacy of our system.Index Terms —Few-Shot learning, Named entity recog-nition, In-context learning, Language model1. INTRODUCTIONNER is a fundamental
framework for the setting of few-shot nested NER. We im-prove the ICL prompt by devising a novel example demon-stration selection mechanism, EnDe retriever. In EnDe re-triever, we employ contrastive learning to perform three typesof representation learning, in terms of semantic similarity,boundary similarity, and label similarity, to generate high-quality demonstration examples. Extensive experiments overthree nested NER and four flat NER datasets demonstrate theefficacy of our system.Index Terms —Few-Shot learning, Named entity recog-nition, In-context learning, Language model1. INTRODUCTIONNER is a fundamental task in natural language processing(NLP) that identifies and classifies entities in unstructuredtext [1]. The evolution of NER has led to the developmentof nested NER [2] from the flat NER, a more intricate vari-ant, where entities can be nested with each other [3]. Unlikeregular NER, nested NER introduces complexity by allowingmultiple entity types to coexist within the same entity token,as exemplified in Fig. 1. Researchers have explored variousapproaches to tackle nested NER, including complex sequen-tial labeling methods [4], graph-based methods [5], and gen
 task in natural language processing(NLP) that identifies and classifies entities in unstructuredtext [1]. The evolution of NER has led to the developmentof nested NER [2] from the flat NER, a more intricate vari-ant, where entities can be nested with each other [3]. Unlikeregular NER, nested NER introduces complexity by allowingmultiple entity types to coexist within the same entity token,as exemplified in Fig. 1. Researchers have explored variousapproaches to tackle nested NER, including complex sequen-tial labeling methods [4], graph-based methods [5], and gen-erative methods [6, 7]. It has been extensively identified byexisting research that the key to nested NER lies in accuratelydelineating the boundaries of nested entities within the text,i.e., when an entity begins and ends [8].Few-shot NER has become pivotal research due to thechallenges posed by the scarcity of labeled data for trainingNER models, especially for the nested scenario. Recent yearshave witnessed the remarkable development of pretrainedLanguage Models (LMs) such as T5 [9] and GPT-3.51. LMs∗Corresponding author: Hao Fei1https://
-erative methods [6, 7]. It has been extensively identified byexisting research that the key to nested NER lies in accuratelydelineating the boundaries of nested entities within the text,i.e., when an entity begins and ends [8].Few-shot NER has become pivotal research due to thechallenges posed by the scarcity of labeled data for trainingNER models, especially for the nested scenario. Recent yearshave witnessed the remarkable development of pretrainedLanguage Models (LMs) such as T5 [9] and GPT-3.51. LMs∗Corresponding author: Hao Fei1https://platform.openai.com/docs/guides/gptLast night, at the Chinese embassy in France, there was a holiday atmosphereFacilityGPE GPEFig. 1 . Illustration of nested NER.have demonstrated exceptional few-shot learning capabili-ties [10], making them promising for tackling few-shot NER[11]. Leveraging the power of LMs as backbones, few-shotNER involves providing LMs with a few example entities andprompting them to predict all possible mentions and labels fora given sentence [12]. The introduction of in-context learning(ICL) [13] has further enhanced the
platform.openai.com/docs/guides/gptLast night, at the Chinese embassy in France, there was a holiday atmosphereFacilityGPE GPEFig. 1 . Illustration of nested NER.have demonstrated exceptional few-shot learning capabili-ties [10], making them promising for tackling few-shot NER[11]. Leveraging the power of LMs as backbones, few-shotNER involves providing LMs with a few example entities andprompting them to predict all possible mentions and labels fora given sentence [12]. The introduction of in-context learning(ICL) [13] has further enhanced the efficacy of LM, whereLMs are provided with label demonstrations and semanticcontext for stronger few-shot NER capabilities.It’s worth noting that, as revealed by existing findings[14], the effectiveness of ICL-based few-shot NER heavily re-lies on the selection of appropriate demonstration examples.Prior research has demonstrated that the choice of appropriateICL examples plays a critical role in prompting LMs to pro-duce accurate and informative NER predictions. This princi-ple equally applies to few-shot nested NER, where selectingthe optimal ICL examples becomes even more challengingdue to the complex entity boundaries. We observe at leastthree key
 efficacy of LM, whereLMs are provided with label demonstrations and semanticcontext for stronger few-shot NER capabilities.It’s worth noting that, as revealed by existing findings[14], the effectiveness of ICL-based few-shot NER heavily re-lies on the selection of appropriate demonstration examples.Prior research has demonstrated that the choice of appropriateICL examples plays a critical role in prompting LMs to pro-duce accurate and informative NER predictions. This princi-ple equally applies to few-shot nested NER, where selectingthe optimal ICL examples becomes even more challengingdue to the complex entity boundaries. We observe at leastthree key challenges: First , the selected ICL examples shouldhave high semantic similarity to the input test sentence. Of-ten, sentences with more identical semantics involve similarentities and similar labels. Second , the entities within thechosen ICL examples should align closely with the bound-aries of entities within the test instance. This alignment is par-ticularly crucial for nested NER, as it relies heavily on bound-ary detection. Third , for nested entities, the labels shouldbe carefully chosen to differentiate between entities that mayshare similar word semantic representations. Even when twonested entities have close word representations, their labelsshould have sufficient differentiation to avoid
 challenges: First , the selected ICL examples shouldhave high semantic similarity to the input test sentence. Of-ten, sentences with more identical semantics involve similarentities and similar labels. Second , the entities within thechosen ICL examples should align closely with the bound-aries of entities within the test instance. This alignment is par-ticularly crucial for nested NER, as it relies heavily on bound-ary detection. Third , for nested entities, the labels shouldbe carefully chosen to differentiate between entities that mayshare similar word semantic representations. Even when twonested entities have close word representations, their labelsshould have sufficient differentiation to avoid ambiguity.Based on the above observations, this work introducesan innovative ICL framework for highly effective few-shotnested NER. In our approach, we first devise an integral ICLprompt template, as illustrated in Fig. 2, which comprisesfour main components: Task Instruction ,Demonstrations ,Label Set andTesting Sentence . The heart of our methodol-arXiv:2402.01182v1 [cs.CL] 2 Feb 2024 Instruction:Do the named entity recognition (NER) task, please extract entity and their types from given sentence based on your knowledge, including those
 ambiguity.Based on the above observations, this work introducesan innovative ICL framework for highly effective few-shotnested NER. In our approach, we first devise an integral ICLprompt template, as illustrated in Fig. 2, which comprisesfour main components: Task Instruction ,Demonstrations ,Label Set andTesting Sentence . The heart of our methodol-arXiv:2402.01182v1 [cs.CL] 2 Feb 2024 Instruction:Do the named entity recognition (NER) task, please extract entity and their types from given sentence based on your knowledge, including those nested entities, where the entity types should be chosen from given labels.Label Set :person, organization, location, facility, gpe ...Demonstrations:In "Last night, at the Chinese embassy in France, there was a holiday atmosphere ",The POS tags are "JJ NN IN DT JJ NN IN NNP ...", and the constituency structure is "(ROOT (S (NP (DT Last) (NN night)) (, ,) (PP (IN at) ...".Based on these feature, "the Chinese embassy in France " is a facility entity, " Chinese " is a gpe entity and " France " is a g
 nested entities, where the entity types should be chosen from given labels.Label Set :person, organization, location, facility, gpe ...Demonstrations:In "Last night, at the Chinese embassy in France, there was a holiday atmosphere ",The POS tags are "JJ NN IN DT JJ NN IN NNP ...", and the constituency structure is "(ROOT (S (NP (DT Last) (NN night)) (, ,) (PP (IN at) ...".Based on these feature, "the Chinese embassy in France " is a facility entity, " Chinese " is a gpe entity and " France " is a gpe entity.Input Sentence:Bank of China corporates with University of Washington … Entity Demo (EnDe) RetrieverLM- "Bank of China" is an organization entity,- "University of Washington" is an organization entity,- "China" is a gpe entity,- "Washington" is a gpe entity.Fig. 2 . Illustration of the prompts with in-context learning.ogy lies in the meticulous selection of demonstrations. Foreach demonstration example, besides the entity labels, wealso enable additional descriptions of NER entity boundaries,including the linguistic part-of-speech (POS) tags, and thesyntactic constituency structure
pe entity.Input Sentence:Bank of China corporates with University of Washington … Entity Demo (EnDe) RetrieverLM- "Bank of China" is an organization entity,- "University of Washington" is an organization entity,- "China" is a gpe entity,- "Washington" is a gpe entity.Fig. 2 . Illustration of the prompts with in-context learning.ogy lies in the meticulous selection of demonstrations. Foreach demonstration example, besides the entity labels, wealso enable additional descriptions of NER entity boundaries,including the linguistic part-of-speech (POS) tags, and thesyntactic constituency structure of the sentence. These fea-tures have been highlighted by prior research to be effectivein accurately defining mention boundaries [15].To identify the most effective examples and offer opti-mal guidance for the test instance, we devise a novel EntityDemo ( EnDe ) Retriever mechanism, which considers twokey aspects during the selection process: a) EnDe selectsexamples with higher comparable sentence semantics to pro-vide better guidance; b) examples with higher degrees ofboundary similarity with the test instance are chosen. Torealize these measurements, we employ contrastive learn-ing [16] for enhanced representation learning, which bring
 of the sentence. These fea-tures have been highlighted by prior research to be effectivein accurately defining mention boundaries [15].To identify the most effective examples and offer opti-mal guidance for the test instance, we devise a novel EntityDemo ( EnDe ) Retriever mechanism, which considers twokey aspects during the selection process: a) EnDe selectsexamples with higher comparable sentence semantics to pro-vide better guidance; b) examples with higher degrees ofboundary similarity with the test instance are chosen. Torealize these measurements, we employ contrastive learn-ing [16] for enhanced representation learning, which bringssentence representations with higher semantic and boundarysimilarity closer in feature space, thus improving few-shotNER performance. Furthermore, we delve deeper into theEnDe Retriever by incorporating a label-centric representa-tion learning, which reduces the feature distance between ICLexamples with the same entity label, and meanwhile increasesthe distance between examples with different labels but tokenoverlap. The experimental results unequivocally demonstratethat our approach consistently achieves new state-of-the-artperformance in few-shot NER across three nested NER andfour flat NER benchmark datasets, validating the robustnessand efficacy of our framework.2. METHODOLOGY
ssentence representations with higher semantic and boundarysimilarity closer in feature space, thus improving few-shotNER performance. Furthermore, we delve deeper into theEnDe Retriever by incorporating a label-centric representa-tion learning, which reduces the feature distance between ICLexamples with the same entity label, and meanwhile increasesthe distance between examples with different labels but tokenoverlap. The experimental results unequivocally demonstratethat our approach consistently achieves new state-of-the-artperformance in few-shot NER across three nested NER andfour flat NER benchmark datasets, validating the robustnessand efficacy of our framework.2. METHODOLOGY2.1. Task DefinitionFormally, we frame the NER task as a problem of classifyingentity spans to encompass all possible word combinations. ToTest Instance ExampleExample1) Semantic s imilarity2) Boundary similarity-- --- -- ---- ----- -- --- ------ --- -- --- ----- ---- -- ---3) Label identicationFacilityChinese GPEFranceGPEthe Chinese embassy in FranceRetrieveReturnPOS TagsConstituency TreeAttractAnnotation PoolFig. 3 . The framework of EnDe Retriever.be specific, given an input sentence xconsisting of nwords,x={w1,···, wn}, we
2.1. Task DefinitionFormally, we frame the NER task as a problem of classifyingentity spans to encompass all possible word combinations. ToTest Instance ExampleExample1) Semantic s imilarity2) Boundary similarity-- --- -- ---- ----- -- --- ------ --- -- --- ----- ---- -- ---3) Label identicationFacilityChinese GPEFranceGPEthe Chinese embassy in FranceRetrieveReturnPOS TagsConstituency TreeAttractAnnotation PoolFig. 3 . The framework of EnDe Retriever.be specific, given an input sentence xconsisting of nwords,x={w1,···, wn}, we generate a set of entity spans, {ei}that consists of a sequence of words ei={wp,···, wq}(where 1≤p≤q≤n) with a specific mention type la-belri∈R(Ris a pre-defined label set), where eican beoverlapped with ejon certain tokens. For the few-shot nestedNER, only a few numbers ( k-shot) of annotated examples areused for the model to learn from.2.2. Generative NER with In-context LearningBased on the generative LM backbone, e.g., T5 or GPT-3.5,
 generate a set of entity spans, {ei}that consists of a sequence of words ei={wp,···, wq}(where 1≤p≤q≤n) with a specific mention type la-belri∈R(Ris a pre-defined label set), where eican beoverlapped with ejon certain tokens. For the few-shot nestedNER, only a few numbers ( k-shot) of annotated examples areused for the model to learn from.2.2. Generative NER with In-context LearningBased on the generative LM backbone, e.g., T5 or GPT-3.5,we transform NER into a text-to-text prediction. We combinethe In-context learning (ICL), which as demonstrated in priorstudies (Brown et al., 2020; Lee et al., 2022b), further im-proves the model’s few-shot capabilities, particularly in NERtasks. As shown in Fig. 2, the prompt comprises four parts:Task Instruction is a prompt that instructs the modelabout the current task it should perform. The instruction for ACE2004 ACE2005 GENIA5-shot 10-shot 20-shot 5-shot 
we transform NER into a text-to-text prediction. We combinethe In-context learning (ICL), which as demonstrated in priorstudies (Brown et al., 2020; Lee et al., 2022b), further im-proves the model’s few-shot capabilities, particularly in NERtasks. As shown in Fig. 2, the prompt comprises four parts:Task Instruction is a prompt that instructs the modelabout the current task it should perform. The instruction for ACE2004 ACE2005 GENIA5-shot 10-shot 20-shot 5-shot 10-shot 20-shot 5-shot 10-shot 20-shotLoc-Lab (Shen et al., 2021) [17] 7.20 22.88 41.02 11.43 25.13 39.61 15.57 31.65 49.67GNER (Yan et al., 2021) [6] 8.87 14.19 28.73 8.7
10-shot 20-shot 5-shot 10-shot 20-shotLoc-Lab (Shen et al., 2021) [17] 7.20 22.88 41.02 11.43 25.13 39.61 15.57 31.65 49.67GNER (Yan et al., 2021) [6] 8.87 14.19 28.73 8.72 13.17 24.26 4.68 10.62 20.98SEE (Yang et al., 2022) [18] 26.54 38.89 48.94 25.58 36.36 51.31 19.31 37.78 50.93SDNet (Chen et al., 2022) [12] 20.55 34.
2 13.17 24.26 4.68 10.62 20.98SEE (Yang et al., 2022) [18] 26.54 38.89 48.94 25.58 36.36 51.31 19.31 37.78 50.93SDNet (Chen et al., 2022) [12] 20.55 34.82 42.87 22.03 32.20 43.00 17.46 19.03 33.27ESD (Wang et al., 2022b) [19] 19.25 42.75 52.17 31.57 38.81 50.30 25.03 35.23 47.22W2NER (Li et al., 20
82 42.87 22.03 32.20 43.00 17.46 19.03 33.27ESD (Wang et al., 2022b) [19] 19.25 42.75 52.17 31.57 38.81 50.30 25.03 35.23 47.22W2NER (Li et al., 2022) [20] 12.52 35.84 45.73 15.48 37.41 43.54 14.25 20.42 34.84FIT (Xu et al., 2023) [21] 35.87 44.88 53.92 37.74 42.25 52.71 34.43 44.95 5
22) [20] 12.52 35.84 45.73 15.48 37.41 43.54 14.25 20.42 34.84FIT (Xu et al., 2023) [21] 35.87 44.88 53.92 37.74 42.25 52.71 34.43 44.95 51.26Ours (T5-base) 40.15 48.43 55.45 44.42 46.10 54.98 39.85 48.27 54.39Table 1 . Main results (F1) on three datasets under different shot sizes.the main NER task is: extracting entity and their types from agiven sentence based on your knowledge .Demonstrations offer input and output examples indemonstrative formats. We also explicitly mark the boundaryfeatures to offer more features
1.26Ours (T5-base) 40.15 48.43 55.45 44.42 46.10 54.98 39.85 48.27 54.39Table 1 . Main results (F1) on three datasets under different shot sizes.the main NER task is: extracting entity and their types from agiven sentence based on your knowledge .Demonstrations offer input and output examples indemonstrative formats. We also explicitly mark the boundaryfeatures to offer more features.Label Set is a list of entity types R, from which the modelneeds to select the corresponding label to annotate the corre-sponding span. The label serves as a constraint and guidancefor entity type selection.Testing Sentence is the input xfrom which entities needto be extracted by LM.2.3. Entity Demonstration (EnDe) RetrieverAs high-quality examples are pivotal to ICL for better prompt-ing LM to induce more correct results, we propose an EnDeretriever. As illustrated in Fig. 3, EnDe retriever performsrepresentation learning based on contrastive learning [16]under three types of
.Label Set is a list of entity types R, from which the modelneeds to select the corresponding label to annotate the corre-sponding span. The label serves as a constraint and guidancefor entity type selection.Testing Sentence is the input xfrom which entities needto be extracted by LM.2.3. Entity Demonstration (EnDe) RetrieverAs high-quality examples are pivotal to ICL for better prompt-ing LM to induce more correct results, we propose an EnDeretriever. As illustrated in Fig. 3, EnDe retriever performsrepresentation learning based on contrastive learning [16]under three types of similarity measurements: semantic simi-larity, boundary similarity and label similarity.2.3.1. Semantic Similarity MeasurementGiven a test sentence xi, EnDe first retrieves from the annota-tion pool (training set) examples with higher sentence seman-tics. Thus, we define the contrastive loss as:LSem=−KXiXi∈Qilogexp [ SimSem(xi||xj)]P∗∈N(i,j)exp [ SimSem(xi||x∗)],(1)where Qicontains all the positive examples that have highsimilarity with xi, i.e., SimSem(a||b)>0
 similarity measurements: semantic simi-larity, boundary similarity and label similarity.2.3.1. Semantic Similarity MeasurementGiven a test sentence xi, EnDe first retrieves from the annota-tion pool (training set) examples with higher sentence seman-tics. Thus, we define the contrastive loss as:LSem=−KXiXi∈Qilogexp [ SimSem(xi||xj)]P∗∈N(i,j)exp [ SimSem(xi||x∗)],(1)where Qicontains all the positive examples that have highsimilarity with xi, i.e., SimSem(a||b)>0.5which is a cosinefunction. N(i, j)contains set of negative pairs.2.3.2. Boundary Similarity MeasurementBoundary features are essential to nested NER, where herewe consider the POS tags and the constituency trees. Also,examples with higher degrees of boundary similarity with thetest instance are chosen. We use LSTM model to encode thePOS tag sequence of a sentence, into sPOSi =LSTM( xPOSi),and likewise, use GCN to encode the constituency tree intorepresentation sConi =GCN( xConi). We then define the con-trastive loss as:
.5which is a cosinefunction. N(i, j)contains set of negative pairs.2.3.2. Boundary Similarity MeasurementBoundary features are essential to nested NER, where herewe consider the POS tags and the constituency trees. Also,examples with higher degrees of boundary similarity with thetest instance are chosen. We use LSTM model to encode thePOS tag sequence of a sentence, into sPOSi =LSTM( xPOSi),and likewise, use GCN to encode the constituency tree intorepresentation sConi =GCN( xConi). We then define the con-trastive loss as:LBdy=LBdyPOS+LBdyCon=−KXiXi∈Qilogexp [ SimBdyPOS(sConi||sConj)]P∗∈N(i,j)exp [ SimBdyPOS(sConi||sCon∗)]−KXiXi∈Qilogexp [ SimBdyCon(sConi||sConj)]P∗∈N(i,j)exp [ SimBdyCon(sConi||sCon∗)].(2)2.3.3. Label Identication MeasurementIt is also important in nested NER that the overlapped to-kens of two nested entities have
LBdy=LBdyPOS+LBdyCon=−KXiXi∈Qilogexp [ SimBdyPOS(sConi||sConj)]P∗∈N(i,j)exp [ SimBdyPOS(sConi||sCon∗)]−KXiXi∈Qilogexp [ SimBdyCon(sConi||sConj)]P∗∈N(i,j)exp [ SimBdyCon(sConi||sCon∗)].(2)2.3.3. Label Identication MeasurementIt is also important in nested NER that the overlapped to-kens of two nested entities have shared word representations.However, the two entities may largely have distinct mentiontypes, i.e., different label semantics. Thus, we further proposea label-centric representation learning, where the contrastiveloss is defined as:LLab=−KXilogexp [ 1(ri, rj)+]P∗∈N(i,j)exp [ 1(ri, r∗)++1(ri, r∗)−],(3)where we put semantically closer those entities in the samelabels, (ri,rj)+.3. EXPERIMENTS3.1. Datasets and SettingsIn line with
 shared word representations.However, the two entities may largely have distinct mentiontypes, i.e., different label semantics. Thus, we further proposea label-centric representation learning, where the contrastiveloss is defined as:LLab=−KXilogexp [ 1(ri, rj)+]P∗∈N(i,j)exp [ 1(ri, r∗)++1(ri, r∗)−],(3)where we put semantically closer those entities in the samelabels, (ri,rj)+.3. EXPERIMENTS3.1. Datasets and SettingsIn line with prior research, we employ standard NER bench-marks, including three nested NER datasets [21] (ACE2004,ACE2005, GENIA) and four flat NER datasets [12] (KBP2017,CoNLL, WNUT, OntoNotes5). Our experiments follow the5/10/20-shot settings as utilized in previous studies. Foreach dataset, we execute ten runs and report the average F1score. Our baselines consist of strong-performing and state-of-the-art few-shot NER systems (all with BERT-base [
 prior research, we employ standard NER bench-marks, including three nested NER datasets [21] (ACE2004,ACE2005, GENIA) and four flat NER datasets [12] (KBP2017,CoNLL, WNUT, OntoNotes5). Our experiments follow the5/10/20-shot settings as utilized in previous studies. Foreach dataset, we execute ten runs and report the average F1score. Our baselines consist of strong-performing and state-of-the-art few-shot NER systems (all with BERT-base [22]or T5-base [9]), as listed in Table 1. For fair comparisons,our backbone is the generative LM, primarily the T5-basemodel, which remains frozen, while updates are confined tothe EnDe retriever. Additionally, we explore LM backbonesof varying sizes and types. We check the F1 metric on the test KBP2017 CoNLL WNUT Onto AvgProto (Huang et al., 2020) 17.3 58.4 29.5 53.3 39.6GNER
22]or T5-base [9]), as listed in Table 1. For fair comparisons,our backbone is the generative LM, primarily the T5-basemodel, which remains frozen, while updates are confined tothe EnDe retriever. Additionally, we explore LM backbonesof varying sizes and types. We check the F1 metric on the test KBP2017 CoNLL WNUT Onto AvgProto (Huang et al., 2020) 17.3 58.4 29.5 53.3 39.6GNER (Yan et al., 2021) 8.4 55.2 24.8 50.4 34.7SEE (Yang et al., 2022) 22.8 67.6 36.2 67.1 48.4W2NER (Li et al., 2022) 18.0 54.5 26.7 48.4 36.9SDNet (Chen et al., 2022) 2
 (Yan et al., 2021) 8.4 55.2 24.8 50.4 34.7SEE (Yang et al., 2022) 22.8 67.6 36.2 67.1 48.4W2NER (Li et al., 2022) 18.0 54.5 26.7 48.4 36.9SDNet (Chen et al., 2022) 20.2 71.4 44.1 71.0 51.6Ours (T5-base) 34.7 73.1 48.3 74.7 57.7Table 2 . 5-shot results on flat NER.5 10 20 50 1002040605 10 20 50 100204060k-shot samplesF1 (%)ACE2004SDNet W2NER OursACE2
0.2 71.4 44.1 71.0 51.6Ours (T5-base) 34.7 73.1 48.3 74.7 57.7Table 2 . 5-shot results on flat NER.5 10 20 50 1002040605 10 20 50 100204060k-shot samplesF1 (%)ACE2004SDNet W2NER OursACE2005Fig. 4 . Results with k-shot samples on two datasets.set, considering a predicted entity as correct if both its entitytype and offsets match the gold entity.3.2. Results on Few-shot Nested NERTable 1 presents the performance of different methods onthree nested datasets. Overall, we find that different few-shotmethods exhibit varied performance increases as the shotsize increases. Notably, models perform significantly bet-ter in the 20-shot setting compared to the 5-shot setting, asincreased examples provide more supervision for learning.Crucially, our model consistently surpasses all baselines by asub
005Fig. 4 . Results with k-shot samples on two datasets.set, considering a predicted entity as correct if both its entitytype and offsets match the gold entity.3.2. Results on Few-shot Nested NERTable 1 presents the performance of different methods onthree nested datasets. Overall, we find that different few-shotmethods exhibit varied performance increases as the shotsize increases. Notably, models perform significantly bet-ter in the 20-shot setting compared to the 5-shot setting, asincreased examples provide more supervision for learning.Crucially, our model consistently surpasses all baselines by asubstantial margin. Remarkably, our model’s improvementsbecome even more pronounced as the shot sizes decrease.In the 5-shot setting, our model’s F1-scores outperform thesecond-best model by large margins. This underscores theeffectiveness of our approach in addressing nested NER taskswith limited supervision.3.3. Results on Few-shot Flat NERIn Fig. 2 we further present the performances on the regularflat NER under few-shot learning. We mainly consider the 5-shot NER. As seen, our system still outperforms all baselineswith clear margins over all datasets, with average 6.
stantial margin. Remarkably, our model’s improvementsbecome even more pronounced as the shot sizes decrease.In the 5-shot setting, our model’s F1-scores outperform thesecond-best model by large margins. This underscores theeffectiveness of our approach in addressing nested NER taskswith limited supervision.3.3. Results on Few-shot Flat NERIn Fig. 2 we further present the performances on the regularflat NER under few-shot learning. We mainly consider the 5-shot NER. As seen, our system still outperforms all baselineswith clear margins over all datasets, with average 6.1% F1 im-provement, where especially ours surpasses the second-bestSEE baseline with 11.9% F1 on KBP2017 data. This furtherindicates that our method’s advance can be compatible to thegeneral few-shot NER setting.3.4. Effects of Shot SizeTo assess our system’s performance across various shot set-tings, we evaluate it alongside two robust baselines, consid-ACE2004 ACE2005 GENIA405060F1 (%)BART-base T5-base T5-large T5-xl GPT3.5Fig
1% F1 im-provement, where especially ours surpasses the second-bestSEE baseline with 11.9% F1 on KBP2017 data. This furtherindicates that our method’s advance can be compatible to thegeneral few-shot NER setting.3.4. Effects of Shot SizeTo assess our system’s performance across various shot set-tings, we evaluate it alongside two robust baselines, consid-ACE2004 ACE2005 GENIA405060F1 (%)BART-base T5-base T5-large T5-xl GPT3.5Fig. 5 . Performances of our system by employing LMs indifferent types and sizes.ering k-shot samples ranging from 5 to 100. Intuitively, moreshot sizes allow better overall performances. As depicted inFig. 4, our system consistently outperforms all others acrossdifferent shot settings. Notably, the enhancements are morepronounced in low-shot settings, affirming the underlyingprinciples of our approach. Further, we find that for oursystem, purely increasing the number of examples leads tolimited performance enhancement. This is largely becauseour EnDe retriever helps already collect the most informativeexamples
. 5 . Performances of our system by employing LMs indifferent types and sizes.ering k-shot samples ranging from 5 to 100. Intuitively, moreshot sizes allow better overall performances. As depicted inFig. 4, our system consistently outperforms all others acrossdifferent shot settings. Notably, the enhancements are morepronounced in low-shot settings, affirming the underlyingprinciples of our approach. Further, we find that for oursystem, purely increasing the number of examples leads tolimited performance enhancement. This is largely becauseour EnDe retriever helps already collect the most informativeexamples, from which the LM benefits the most.3.5. Impacts of Using different LMsFinally, we study the influences of adopting different sizesand types of backbone LMs. We experiment on three datasets,with five different LMs, including BART-base, T5-base, T5-large, T5-xl, and GPT-3.5. Fig. 5 plots the patterns. Wecan find that with the increase of LM sizes, i.e., from T5-base to T5-xl, the performances grow steadily. When usingthe current state-of-the-art GPT-3.5 LM (with
, from which the LM benefits the most.3.5. Impacts of Using different LMsFinally, we study the influences of adopting different sizesand types of backbone LMs. We experiment on three datasets,with five different LMs, including BART-base, T5-base, T5-large, T5-xl, and GPT-3.5. Fig. 5 plots the patterns. Wecan find that with the increase of LM sizes, i.e., from T5-base to T5-xl, the performances grow steadily. When usingthe current state-of-the-art GPT-3.5 LM (with 175B), we findthere is a huge performance leap consistently. This is intuitiveas larger LM shows better unsupervised inference capability.4. CONCLUSIONThis paper introduces an innovative in-context learningframework to address the less-explored few-shot nested NERtask. We improve the ICL prompt by introducing an efficientexample demonstration selection mechanism known as theEnDe retriever. In the EnDe retriever, we employ contrastivelearning to perform three types of representation learningmeasurements, which include semantic similarity, bound-ary similarity, and label similarity, to generate high-qualitydemonstration examples. Comprehensive experiments con-
 175B), we findthere is a huge performance leap consistently. This is intuitiveas larger LM shows better unsupervised inference capability.4. CONCLUSIONThis paper introduces an innovative in-context learningframework to address the less-explored few-shot nested NERtask. We improve the ICL prompt by introducing an efficientexample demonstration selection mechanism known as theEnDe retriever. In the EnDe retriever, we employ contrastivelearning to perform three types of representation learningmeasurements, which include semantic similarity, bound-ary similarity, and label similarity, to generate high-qualitydemonstration examples. Comprehensive experiments con-ducted on three nested NER and four flat NER datasetsconfirm the effectiveness of our system.5. ACKNOWLEDGEMENTThis work is supported by the National Natural Science Foun-dation of China (Grant No. 62176180), and CCF-Baidu OpenFund. 6. REFERENCES[1] Erik F. Tjong Kim Sang and Fien De Meulder, “In-troduction to the CoNLL-2003 shared task: Language-independent named entity recognition,” in Proceedingsof NAACL , 2003, pp. 142–
ducted on three nested NER and four flat NER datasetsconfirm the effectiveness of our system.5. ACKNOWLEDGEMENTThis work is supported by the National Natural Science Foun-dation of China (Grant No. 62176180), and CCF-Baidu OpenFund. 6. REFERENCES[1] Erik F. Tjong Kim Sang and Fien De Meulder, “In-troduction to the CoNLL-2003 shared task: Language-independent named entity recognition,” in Proceedingsof NAACL , 2003, pp. 142–147.[2] Jenny Rose Finkel and Christopher D. Manning,“Nested named entity recognition,” in Proceedings ofEMNLP , 2009, pp. 141–150.[3] Hao Fei, Yafeng Ren, and Donghong Ji, “Boundariesand edges rethinking: An end-to-end neural model foroverlapping entity relation extraction,” Information Pro-cessing & Management , vol. 57, no. 6, pp. 102311,2020.[4] Meizhi Ju, Makoto Mi
147.[2] Jenny Rose Finkel and Christopher D. Manning,“Nested named entity recognition,” in Proceedings ofEMNLP , 2009, pp. 141–150.[3] Hao Fei, Yafeng Ren, and Donghong Ji, “Boundariesand edges rethinking: An end-to-end neural model foroverlapping entity relation extraction,” Information Pro-cessing & Management , vol. 57, no. 6, pp. 102311,2020.[4] Meizhi Ju, Makoto Miwa, and Sophia Ananiadou, “Aneural layered model for nested named entity recogni-tion,” in Proceedings of NAACL , 2018, pp. 1446–1459.[5] Wei Lu and Dan Roth, “Joint mention extraction andclassification with mention hypergraphs,” in Proceed-ings of EMNLP , 2015, pp. 857–867.[6] Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, ZhengZhang, and Xipeng Qiu, “A unified generative
wa, and Sophia Ananiadou, “Aneural layered model for nested named entity recogni-tion,” in Proceedings of NAACL , 2018, pp. 1446–1459.[5] Wei Lu and Dan Roth, “Joint mention extraction andclassification with mention hypergraphs,” in Proceed-ings of EMNLP , 2015, pp. 857–867.[6] Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, ZhengZhang, and Xipeng Qiu, “A unified generative frame-work for various NER subtasks,” in Proceedings of ACL ,2021, pp. 5808–5822.[7] Hao Fei, Shengqiong Wu, Jingye Li, Bobo Li, FeiLi, Libo Qin, Meishan Zhang, Min Zhang, and Tat-Seng Chua, “Lasuie: Unifying information extractionwith latent adaptive structure-aware generative languagemodel,” in Proceedings of NeurIPS , 2022.[8] Hao Fei, Donghong Ji, Bobo Li, Yijiang Liu
 frame-work for various NER subtasks,” in Proceedings of ACL ,2021, pp. 5808–5822.[7] Hao Fei, Shengqiong Wu, Jingye Li, Bobo Li, FeiLi, Libo Qin, Meishan Zhang, Min Zhang, and Tat-Seng Chua, “Lasuie: Unifying information extractionwith latent adaptive structure-aware generative languagemodel,” in Proceedings of NeurIPS , 2022.[8] Hao Fei, Donghong Ji, Bobo Li, Yijiang Liu, YafengRen, and Fei Li, “Rethinking boundaries: End-to-endrecognition of discontinuous mentions with pointer net-works,” in Proceedings of AAAI , 2021, pp. 12785–12793.[9] Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Michael Matena, Yanqi Zhou, Wei Li, and Peter J.Liu, “Exploring the limits of transfer learning with aunified text-to-text transformer,” Journal of MachineLearning Research , vol. 21, no. 14
, YafengRen, and Fei Li, “Rethinking boundaries: End-to-endrecognition of discontinuous mentions with pointer net-works,” in Proceedings of AAAI , 2021, pp. 12785–12793.[9] Colin Raffel, Noam Shazeer, Adam Roberts, KatherineLee, Michael Matena, Yanqi Zhou, Wei Li, and Peter J.Liu, “Exploring the limits of transfer learning with aunified text-to-text transformer,” Journal of MachineLearning Research , vol. 21, no. 140, pp. 1–67, 2020.[10] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua, “Next-gpt: Any-to-any multimodal llm,”2023.[11] Jiaxin Huang, Chunyuan Li, Krishan Subudhi, DamienJose, Shobana Balakrishnan, Weizhu Chen, BaolinPeng, Jianfeng Gao, and Jiawei Han, “Few-shot namedentity recognition: A comprehensive study,” CoRR ,
0, pp. 1–67, 2020.[10] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua, “Next-gpt: Any-to-any multimodal llm,”2023.[11] Jiaxin Huang, Chunyuan Li, Krishan Subudhi, DamienJose, Shobana Balakrishnan, Weizhu Chen, BaolinPeng, Jianfeng Gao, and Jiawei Han, “Few-shot namedentity recognition: A comprehensive study,” CoRR , vol.abs/2012.14978, 2020.[12] Jiawei Chen, Qing Liu, Hongyu Lin, Xianpei Han, andLe Sun, “Few-shot named entity recognition with self-describing networks,” in Proceedings of ACL , 2022, pp.5711–5722.[13] Tom B. Brown, Benjamin Mann, Ilya Sutskever, andDario Amodei, “Language models are few-shot learn-ers,” in NeurIPS , 2020.[14] Sew
 vol.abs/2012.14978, 2020.[12] Jiawei Chen, Qing Liu, Hongyu Lin, Xianpei Han, andLe Sun, “Few-shot named entity recognition with self-describing networks,” in Proceedings of ACL , 2022, pp.5711–5722.[13] Tom B. Brown, Benjamin Mann, Ilya Sutskever, andDario Amodei, “Language models are few-shot learn-ers,” in NeurIPS , 2020.[14] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-moyer, “Rethinking the role of demonstrations: Whatmakes in-context learning work?,” in Proceedings ofEMNLP , 2022, pp. 11048–11064.[15] Hao Fei, Shengqiong Wu, Yafeng Ren, Fei Li, andDonghong Ji, “Better combine them together! integrat-ing syntactic constituency and dependency representa-tions
on Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-moyer, “Rethinking the role of demonstrations: Whatmakes in-context learning work?,” in Proceedings ofEMNLP , 2022, pp. 11048–11064.[15] Hao Fei, Shengqiong Wu, Yafeng Ren, Fei Li, andDonghong Ji, “Better combine them together! integrat-ing syntactic constituency and dependency representa-tions for semantic role labeling,” in Findings of the ACL ,2021, pp. 549–559.[16] Sumit Chopra, Raia Hadsell, and Yann LeCun, “Learn-ing a similarity metric discriminatively, with applicationto face verification,” in Proceedings of CVPR , 2005,vol. 1, pp. 539–546.[17] Yongliang Shen, Xinyin Ma, Zeqi Tan, Shuai Zhang,Wen Wang, and Weiming Lu, “Locate and label: A two
 for semantic role labeling,” in Findings of the ACL ,2021, pp. 549–559.[16] Sumit Chopra, Raia Hadsell, and Yann LeCun, “Learn-ing a similarity metric discriminatively, with applicationto face verification,” in Proceedings of CVPR , 2005,vol. 1, pp. 539–546.[17] Yongliang Shen, Xinyin Ma, Zeqi Tan, Shuai Zhang,Wen Wang, and Weiming Lu, “Locate and label: A two-stage identifier for nested named entity recognition,” inProceedings of ACL , 2021, pp. 2782–2794.[18] Zeng Yang, Linhai Zhang, and Deyu Zhou, “SEE-few: Seed, expand and entail for few-shot named en-tity recognition,” in Proceedings of COLING , 2022, pp.2540–2550.[19] Peiyi Wang, Runxin Xu, Tianyu Liu, Qingyu Zhou,Yunbo Cao, Baobao Chang, and Zhifang Sui
-stage identifier for nested named entity recognition,” inProceedings of ACL , 2021, pp. 2782–2794.[18] Zeng Yang, Linhai Zhang, and Deyu Zhou, “SEE-few: Seed, expand and entail for few-shot named en-tity recognition,” in Proceedings of COLING , 2022, pp.2540–2550.[19] Peiyi Wang, Runxin Xu, Tianyu Liu, Qingyu Zhou,Yunbo Cao, Baobao Chang, and Zhifang Sui, “An en-hanced span-based decomposition method for few-shotsequence labeling,” in Proceedings of NAACL , 2022, pp.5012–5024.[20] Jingye Li, Hao Fei, Jiang Liu, Meishan Zhang, ChongTeng, Donghong Ji, and Fei Li, “Unified named en-tity recognition as word-word relation classification,” inProceedings of AAAI , 2022, pp. 10965–10973.[21] Yuanyuan Xu, Zeng Yang, Lin
, “An en-hanced span-based decomposition method for few-shotsequence labeling,” in Proceedings of NAACL , 2022, pp.5012–5024.[20] Jingye Li, Hao Fei, Jiang Liu, Meishan Zhang, ChongTeng, Donghong Ji, and Fei Li, “Unified named en-tity recognition as word-word relation classification,” inProceedings of AAAI , 2022, pp. 10965–10973.[21] Yuanyuan Xu, Zeng Yang, Linhai Zhang, Deyu Zhou,Tiandeng Wu, and Rong Zhou, “Focusing, bridgingand prompting for few-shot nested named entity recog-nition,” in Findings of ACL , 2023, pp. 2621–2637.[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova, “BERT: Pre-training of deep bidi-rectional transformers for language understanding,” inProceedings of NAACL , 2019, pp. 4171–4186.
hai Zhang, Deyu Zhou,Tiandeng Wu, and Rong Zhou, “Focusing, bridgingand prompting for few-shot nested named entity recog-nition,” in Findings of ACL , 2023, pp. 2621–2637.[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova, “BERT: Pre-training of deep bidi-rectional transformers for language understanding,” inProceedings of NAACL , 2019, pp. 4171–4186.
