A Novel Global Feature-Oriented Relational Triple Extraction Modelbased on Table FillingFeiliang Ren†,∗, Longhui Zhang†, Shujuan Yin, Xiaofeng Zhao, Shilei LiuBochao Li ,Yaduo LiuSchool of Computer Science and EngineeringKey Laboratory of Medical Image Computing of Ministry of EducationNortheastern University, Shenyang, 110169, Chinarenfeiliang@cse.neu.edu.cnAbstractTable ﬁlling based relational triple extractionmethods are attracting growing research inter-ests due to their promising performance andtheir abilities on extracting triples from com-plex sentences. However, this kind of methodsare far from their full potential because most ofthem only focus on using local features but ig-nore the global associations of relations and oftoken pairs, which increases the possibility ofoverlooking some important information dur-ing triple extraction. To overcome this deﬁ-ciency, we propose a global feature-orientedtriple extraction model that makes full use ofthe mentioned two kinds of global associations.Speciﬁcally, we ﬁrst generate a table featurefor each relation. Then two kinds of globalassociations are mined from the generated ta-ble features. Next, the mined global associ-
 com-plex sentences. However, this kind of methodsare far from their full potential because most ofthem only focus on using local features but ig-nore the global associations of relations and oftoken pairs, which increases the possibility ofoverlooking some important information dur-ing triple extraction. To overcome this deﬁ-ciency, we propose a global feature-orientedtriple extraction model that makes full use ofthe mentioned two kinds of global associations.Speciﬁcally, we ﬁrst generate a table featurefor each relation. Then two kinds of globalassociations are mined from the generated ta-ble features. Next, the mined global associ-ations are integrated into the table feature ofeach relation. This “ generate-mine-integrate ”process is performed multiple times so thatthe table feature of each relation is reﬁnedstep by step. Finally, each relation’s table isﬁlled based on its reﬁned table feature, andall triples linked to this relation are extractedbased on its ﬁlled table. We evaluate the pro-posed model on three benchmark datasets. Ex-perimental results show our model is effectiveand it achieves state-of-the-art results on all ofthese datasets. The source code of our work isavailable at: https://github.com/neuk
ations are integrated into the table feature ofeach relation. This “ generate-mine-integrate ”process is performed multiple times so thatthe table feature of each relation is reﬁnedstep by step. Finally, each relation’s table isﬁlled based on its reﬁned table feature, andall triples linked to this relation are extractedbased on its ﬁlled table. We evaluate the pro-posed model on three benchmark datasets. Ex-perimental results show our model is effectiveand it achieves state-of-the-art results on all ofthese datasets. The source code of our work isavailable at: https://github.com/neukg/GRTE.1 IntroductionRelational triple extraction (RTE) aims to extracttriples from unstructured text (often sentences), andis a fundamental task in information extraction.These triples have the form of ( subject, relation,object ) , where both subject andobject are entitiesand they are semantically linked by relation . RTEis important for many downstream applications like†Both authors contribute equally to this work and shareco-ﬁrst authorship.∗Corresponding author.automatic knowledge graph construction, etc.Nowadays, the dominant methods for RTE arethe joint extraction methods that extract entitiesand relations simultaneously in an end-to-end
g/GRTE.1 IntroductionRelational triple extraction (RTE) aims to extracttriples from unstructured text (often sentences), andis a fundamental task in information extraction.These triples have the form of ( subject, relation,object ) , where both subject andobject are entitiesand they are semantically linked by relation . RTEis important for many downstream applications like†Both authors contribute equally to this work and shareco-ﬁrst authorship.∗Corresponding author.automatic knowledge graph construction, etc.Nowadays, the dominant methods for RTE arethe joint extraction methods that extract entitiesand relations simultaneously in an end-to-end way.Some latest joint extraction methods (Yu et al.,2019; Yuan et al., 2020; Zeng et al., 2020; Weiet al., 2020; Wang et al., 2020; Sun et al., 2021)have shown their strong extraction abilities on di-verse benchmark datasets, especially the abilitiesof extracting triples from complex sentences thatcontain overlapping or multiple triples.Among these existing joint extraction methods,a kind of table ﬁlling based methods (Wang et al.,2020; Zhang et al., 
 way.Some latest joint extraction methods (Yu et al.,2019; Yuan et al., 2020; Zeng et al., 2020; Weiet al., 2020; Wang et al., 2020; Sun et al., 2021)have shown their strong extraction abilities on di-verse benchmark datasets, especially the abilitiesof extracting triples from complex sentences thatcontain overlapping or multiple triples.Among these existing joint extraction methods,a kind of table ﬁlling based methods (Wang et al.,2020; Zhang et al., 2017; Miwa and Bansal, 2016;Gupta et al., 2016) are attracting growing researchattention. These methods usually maintain a tablefor each relation, and each item in such a tableis used to indicate whether a token pair possessthe corresponding relation or not. Thus the keyof these methods is to ﬁll the relation tables accu-rately, then the triples can be extracted based onthe ﬁlled tables. However, existing methods ﬁllrelation tables mainly based on local features thatare extracted from either a single token pair (Wan
2017; Miwa and Bansal, 2016;Gupta et al., 2016) are attracting growing researchattention. These methods usually maintain a tablefor each relation, and each item in such a tableis used to indicate whether a token pair possessthe corresponding relation or not. Thus the keyof these methods is to ﬁll the relation tables accu-rately, then the triples can be extracted based onthe ﬁlled tables. However, existing methods ﬁllrelation tables mainly based on local features thatare extracted from either a single token pair (Wanget al., 2020) or the ﬁlled history of some limitedtoken pairs (Zhang et al., 2017), but ignore fol-lowing two kinds of valuable global features: theglobal associations of token pairs and of relations.These two kinds of global associations can re-veal the differences and connections among rela-tions and among token pairs. Thus they are helpfulto the precision by verifying the extracted triplesfrom multiple perspectives, and are also helpful tothe recall by deducing new triples. For example,given a sentence “ Edward Thomas and John arefrom New York City, USA
get al., 2020) or the ﬁlled history of some limitedtoken pairs (Zhang et al., 2017), but ignore fol-lowing two kinds of valuable global features: theglobal associations of token pairs and of relations.These two kinds of global associations can re-veal the differences and connections among rela-tions and among token pairs. Thus they are helpfulto the precision by verifying the extracted triplesfrom multiple perspectives, and are also helpful tothe recall by deducing new triples. For example,given a sentence “ Edward Thomas and John arefrom New York City, USA. ”, when looking it from aglobal view, we can easily ﬁnd following two use-ful facts. First, the triple ( Edward Thomas, live_in,New York ) is helpful for extracting the triple ( John,live_in, USA ), and vice versa. This is because theentity pair ( John ,USA) is highly possible to pos- sess the same relation as the entity pair ( EdwardThomas ,New York ) since: (i) the entity types oftheir head entities are same, and (ii) the entity typesof their tail entities are same too. Second, the men-tioned two triples are helpful
. ”, when looking it from aglobal view, we can easily ﬁnd following two use-ful facts. First, the triple ( Edward Thomas, live_in,New York ) is helpful for extracting the triple ( John,live_in, USA ), and vice versa. This is because theentity pair ( John ,USA) is highly possible to pos- sess the same relation as the entity pair ( EdwardThomas ,New York ) since: (i) the entity types oftheir head entities are same, and (ii) the entity typesof their tail entities are same too. Second, the men-tioned two triples are helpful for deducing a newtriple ( New York, located_in, USA ). This is becausethat: (i) live_in indicates its tail entities are loca-tions , and (ii) located_in andlive_in are seman-tically relevant, and both of them are related tolocations . Obviously, both kinds of global featuresare impossible to be contained in local features.Inspired by above analyses, we propose a globalfeature-oriented table ﬁlling based RTE model thatﬁll tables mainly based on above two kinds ofglobal associations. In our model, we ﬁrst generatea table feature for each
 for deducing a newtriple ( New York, located_in, USA ). This is becausethat: (i) live_in indicates its tail entities are loca-tions , and (ii) located_in andlive_in are seman-tically relevant, and both of them are related tolocations . Obviously, both kinds of global featuresare impossible to be contained in local features.Inspired by above analyses, we propose a globalfeature-oriented table ﬁlling based RTE model thatﬁll tables mainly based on above two kinds ofglobal associations. In our model, we ﬁrst generatea table feature for each relation. Then all relations’table features are integrated into a subject-relatedglobal feature and an object-related global feature,based on which two kinds of global associationsare mined with a Transformer -based method. Next,these two kinds of mined global associations areused to reﬁne the table features. These steps areperformed multiple times so that the table featuresare reﬁned gradually. Finally, each table is ﬁlledbased on its reﬁned table feature, and all triples areextracted based on the ﬁlled tables.We evaluate the proposed model on three bench-mark datasets: NYT29, NYT24,
 relation. Then all relations’table features are integrated into a subject-relatedglobal feature and an object-related global feature,based on which two kinds of global associationsare mined with a Transformer -based method. Next,these two kinds of mined global associations areused to reﬁne the table features. These steps areperformed multiple times so that the table featuresare reﬁned gradually. Finally, each table is ﬁlledbased on its reﬁned table feature, and all triples areextracted based on the ﬁlled tables.We evaluate the proposed model on three bench-mark datasets: NYT29, NYT24, and WebNLG.Extensive experiments show that it consistentlyoutperforms the existing best models and achievesthe state-of-the-art results on all of these datasets.2 Related WorkEarly study (Zelenko et al., 2003; Zhou et al., 2005;Chan and Roth, 2011) often takes a kind of pipelinebased methods for RTE, which is to recognize allentities in the input text ﬁrst and then to predictthe relations for all entity pairs. However, thesemethods have two fatal shortcomings. First, theyignore the correlations between entity recognitionand relation prediction.
 and WebNLG.Extensive experiments show that it consistentlyoutperforms the existing best models and achievesthe state-of-the-art results on all of these datasets.2 Related WorkEarly study (Zelenko et al., 2003; Zhou et al., 2005;Chan and Roth, 2011) often takes a kind of pipelinebased methods for RTE, which is to recognize allentities in the input text ﬁrst and then to predictthe relations for all entity pairs. However, thesemethods have two fatal shortcomings. First, theyignore the correlations between entity recognitionand relation prediction. Second, they tend to sufferfrom the error propagation issue.To overcome these shortcomings, researchersbegin to explore the joint extraction methods thatextract entities and relations simultaneously. Ac-cording to the research lines taken, we roughly clas-sify existing joint methods into three main kinds.Tagging based methods . This kind of methods(Zheng et al., 2017; Yu et al., 2019; Wei et al., 2020)often ﬁrst extract the entities by a tagging basedmethod, then predict relations. In these models, bi-.... New Y ork City ,USAEdward Thomas and John
 Second, they tend to sufferfrom the error propagation issue.To overcome these shortcomings, researchersbegin to explore the joint extraction methods thatextract entities and relations simultaneously. Ac-cording to the research lines taken, we roughly clas-sify existing joint methods into three main kinds.Tagging based methods . This kind of methods(Zheng et al., 2017; Yu et al., 2019; Wei et al., 2020)often ﬁrst extract the entities by a tagging basedmethod, then predict relations. In these models, bi-.... New Y ork City ,USAEdward Thomas and John ....N/A MMH N/AN/A N/A MMTN/AMMTN/AN/AMSHMSTN/A N/A N/AN/A SMH SMTN/ASMTN/AN/AN/ASSN/A N/A N/A N/A N/A N/AFigure 1: Examples of table ﬁlling and decodingstrategy. Arrows with different colors correspond todifferent search routes deﬁned in Algorithm 1.nary tagging sequences are often used to determinethe start and end positions of entities, sometimes todetermine the relations between two entities too.Seq2Seq based methods . This kinds
 ....N/A MMH N/AN/A N/A MMTN/AMMTN/AN/AMSHMSTN/A N/A N/AN/A SMH SMTN/ASMTN/AN/AN/ASSN/A N/A N/A N/A N/A N/AFigure 1: Examples of table ﬁlling and decodingstrategy. Arrows with different colors correspond todifferent search routes deﬁned in Algorithm 1.nary tagging sequences are often used to determinethe start and end positions of entities, sometimes todetermine the relations between two entities too.Seq2Seq based methods . This kinds of meth-ods (Zeng et al., 2018, 2019, 2020; Nayak and Ng,2020) often view a triple as a token sequence, andconvert the extraction task into a generation taskthat generates a triple in some orders, such as ﬁrstgenerates a relation, then generates entities, etc.Table ﬁlling based methods . This kind of meth-ods (Miwa and Bansal, 2016; Gupta et al., 2016;Zhang et al., 2017; Wang
 of meth-ods (Zeng et al., 2018, 2019, 2020; Nayak and Ng,2020) often view a triple as a token sequence, andconvert the extraction task into a generation taskthat generates a triple in some orders, such as ﬁrstgenerates a relation, then generates entities, etc.Table ﬁlling based methods . This kind of meth-ods (Miwa and Bansal, 2016; Gupta et al., 2016;Zhang et al., 2017; Wang et al., 2020) would main-tain a table for each relation, and the items in thistable usually denotes the start and end positionsof two entities (or even the types of these entities)that possess this speciﬁc relation. Accordingly, theRTE task is converted into the task of ﬁlling thesetables accurately and effectively.Besides, researchers also explore other kindsof methods. For example, Bekoulis et al. (2018)formulate the RTE task as a multi-head selectionproblem. Li et al. (2019) cast
 et al., 2020) would main-tain a table for each relation, and the items in thistable usually denotes the start and end positionsof two entities (or even the types of these entities)that possess this speciﬁc relation. Accordingly, theRTE task is converted into the task of ﬁlling thesetables accurately and effectively.Besides, researchers also explore other kindsof methods. For example, Bekoulis et al. (2018)formulate the RTE task as a multi-head selectionproblem. Li et al. (2019) cast the RTE task as amulti-turn question answering problem. Fu et al.(2019) use a graph convolutional networks basedmethod and Eberts and Ulges (2019) use a spanextraction based method. Sun et al. (2021) proposea multitask learning based RTE model.3 Methodology3.1 Table Filling StrategyGiven a sentence S=w1w2. . . w n, we will main-tain a table table r(the size is n×n) for each rela-tionr(r∈R, andRis the relation set). The core
 the RTE task as amulti-turn question answering problem. Fu et al.(2019) use a graph convolutional networks basedmethod and Eberts and Ulges (2019) use a spanextraction based method. Sun et al. (2021) proposea multitask learning based RTE model.3 Methodology3.1 Table Filling StrategyGiven a sentence S=w1w2. . . w n, we will main-tain a table table r(the size is n×n) for each rela-tionr(r∈R, andRis the relation set). The coreof our model is to assign a proper label for each w1w2wn …Encoder TFGMaxpooling&FFNTF(t)TG All TriplesGFMTF(N)H(t+1)s/oH(t)s/o…H(1)sH(1)oH…Multi-Head AttentionMulti-Head AttentionFFNK VK V QQ…H(t+1)s/o×(N−1)Figure 2: Model Architecture. The dotted arrows to TFG means that H(1)sandH(1)owill be inputted to TFG onlyat the ﬁrst iteration. The dotted arrow
of our model is to assign a proper label for each w1w2wn …Encoder TFGMaxpooling&FFNTF(t)TG All TriplesGFMTF(N)H(t+1)s/oH(t)s/o…H(1)sH(1)oH…Multi-Head AttentionMulti-Head AttentionFFNK VK V QQ…H(t+1)s/o×(N−1)Figure 2: Model Architecture. The dotted arrows to TFG means that H(1)sandH(1)owill be inputted to TFG onlyat the ﬁrst iteration. The dotted arrow to TGmeans that TF(N)will be inputted into TGonly at the last iteration.table item (corresponding to a token pair). Herewe deﬁne the label set as L= {"N/A", "MMH","MMT", "MSH", "MST", "SMH", "SMT", "SS"}.For a token pair indexed by the i-th row and thej-th column, we denote it as ( wi,wj) and denoteits label as l. Ifl∈{"MMH", "MMT", "MSH","MST", "SMH", "SMT
 to TGmeans that TF(N)will be inputted into TGonly at the last iteration.table item (corresponding to a token pair). Herewe deﬁne the label set as L= {"N/A", "MMH","MMT", "MSH", "MST", "SMH", "SMT", "SS"}.For a token pair indexed by the i-th row and thej-th column, we denote it as ( wi,wj) and denoteits label as l. Ifl∈{"MMH", "MMT", "MSH","MST", "SMH", "SMT"}, it means ( wi,wj) iscorrelated with a ( subject, object ) pair. In suchcase, the ﬁrst character in the label refers to thesubject is either a multi-token entity ("M") or asingle-token entity ("S"), the second character inthe label refers to the object is either a multi-tokenentity ("M") or a single-token entity ("S"), and thethird character in the label refers to either both wiandwjare the head token of the subject andobject("H") or both are the tail token of the subject andobject ("T"). For example, l= "MMH
"}, it means ( wi,wj) iscorrelated with a ( subject, object ) pair. In suchcase, the ﬁrst character in the label refers to thesubject is either a multi-token entity ("M") or asingle-token entity ("S"), the second character inthe label refers to the object is either a multi-tokenentity ("M") or a single-token entity ("S"), and thethird character in the label refers to either both wiandwjare the head token of the subject andobject("H") or both are the tail token of the subject andobject ("T"). For example, l= "MMH" means wiis the head token of a multi-token subject and wjisthe head token of a multi-token object. As for othercases, if l= "SS", it means ( wi,wj) is an entitypair; if l= "N/A", it means wiandwjare none ofabove cases.Figure 1 demonstrates partial ﬁlled results forthelive_in relation given the sentence " EdwardThomas and John are from New York City, USA. ",where there are ( subject, object ) pairs of ( EdwardThomas ,New York City ), (Edward Thomas ,NewYork), (Edward Thomas ,USA
" means wiis the head token of a multi-token subject and wjisthe head token of a multi-token object. As for othercases, if l= "SS", it means ( wi,wj) is an entitypair; if l= "N/A", it means wiandwjare none ofabove cases.Figure 1 demonstrates partial ﬁlled results forthelive_in relation given the sentence " EdwardThomas and John are from New York City, USA. ",where there are ( subject, object ) pairs of ( EdwardThomas ,New York City ), (Edward Thomas ,NewYork), (Edward Thomas ,USA), (John ,New YorkCity), (John ,New York ) and ( John ,USA).An main merit of our ﬁlling strategy is that eachof its label can not only reveal the location informa-tion of a token in a subject or an object, but also canreveal whether a subject (or an object) is a singletoken entity or multi token entity. Thus, the totalnumber of items to be ﬁlled under our ﬁlling strat-egy is generally small since the information carriedby each label increases. For example, given a sen-tence S=w1w2.
), (John ,New YorkCity), (John ,New York ) and ( John ,USA).An main merit of our ﬁlling strategy is that eachof its label can not only reveal the location informa-tion of a token in a subject or an object, but also canreveal whether a subject (or an object) is a singletoken entity or multi token entity. Thus, the totalnumber of items to be ﬁlled under our ﬁlling strat-egy is generally small since the information carriedby each label increases. For example, given a sen-tence S=w1w2. . . w nand a relation set R, thenumber of items to be ﬁlled under our ﬁlling strat-egy is n2|R|, while this number is (2|R|+ 1)n2+n2under the ﬁlling strategy used in TPLinker (Wanget al., 2020) (this number is copied from the orig-inal paper of TPLinker directly). One can easilydeduce that (2|R|+ 1)n2+n2> n2|R|.3.2 Model DetailsThe architecture of our model is shown in Figure
 . . w nand a relation set R, thenumber of items to be ﬁlled under our ﬁlling strat-egy is n2|R|, while this number is (2|R|+ 1)n2+n2under the ﬁlling strategy used in TPLinker (Wanget al., 2020) (this number is copied from the orig-inal paper of TPLinker directly). One can easilydeduce that (2|R|+ 1)n2+n2> n2|R|.3.2 Model DetailsThe architecture of our model is shown in Figure 2.It consists of four main modules: an Encoder mod-ule, a Table Feature Generation (TFG ) module,aGlobal Feature Mining (GFM ) module, and aTriple Generation (TG) module. TFG andGFMare performed multiple time with an iterative wayso that the table features are reﬁned step by step.Finally, TGﬁlls each table based on its correspond-ing reﬁned table feature and generates all triplesbased on these ﬁlled tables.Encoder Module Here a pre-trained BERT-Base(Cased) model (Devlin et al., 2018) is used
 2.It consists of four main modules: an Encoder mod-ule, a Table Feature Generation (TFG ) module,aGlobal Feature Mining (GFM ) module, and aTriple Generation (TG) module. TFG andGFMare performed multiple time with an iterative wayso that the table features are reﬁned step by step.Finally, TGﬁlls each table based on its correspond-ing reﬁned table feature and generates all triplesbased on these ﬁlled tables.Encoder Module Here a pre-trained BERT-Base(Cased) model (Devlin et al., 2018) is used as En-coder . Given a sentence, this module ﬁrstly en-codes it into a token representation sequence (de-noted as H∈Rn×dh).Then His fed into two separated Feed-ForwardNetworks (FFN ) to generate the initial subjectsfeature and objects feature (denoted as H(1)sandH(1)orespectively), as written with Eq. (1).H(1)s=W1H+b1H(1)o=W2H+b2(1) where W1/2∈Rdh×dhare trainable weights andb1/2∈Rdhare
 as En-coder . Given a sentence, this module ﬁrstly en-codes it into a token representation sequence (de-noted as H∈Rn×dh).Then His fed into two separated Feed-ForwardNetworks (FFN ) to generate the initial subjectsfeature and objects feature (denoted as H(1)sandH(1)orespectively), as written with Eq. (1).H(1)s=W1H+b1H(1)o=W2H+b2(1) where W1/2∈Rdh×dhare trainable weights andb1/2∈Rdhare trainable biases.TFG Module We denote the subjects andobjectsfeatures at the t-thiteration as H(t)sandH(t)ore-spectively. Then taking them as input, this modulegenerates a table feature for each relation.Here the table feature for the relation rat thet-thiteration is denoted as TF(t)r, and it has thesame size with table r. Each item in TF(t)rrepre-sents the label feature for a token pair. Speciﬁcally,for a pair (wi, wj), we denoted its label feature asTF(t)r(i, j), which is computed with Eq
 trainable biases.TFG Module We denote the subjects andobjectsfeatures at the t-thiteration as H(t)sandH(t)ore-spectively. Then taking them as input, this modulegenerates a table feature for each relation.Here the table feature for the relation rat thet-thiteration is denoted as TF(t)r, and it has thesame size with table r. Each item in TF(t)rrepre-sents the label feature for a token pair. Speciﬁcally,for a pair (wi, wj), we denoted its label feature asTF(t)r(i, j), which is computed with Eq.(2).TF(t)r(i, j) =WrReLU( H(t)s,i◦H(t)o,j) +br(2)where◦denotes the Hadamard Product operation,ReLU is the activation function, H(t)s,iandH(t)o,jarethe feature representations of tokens wiandwjatthet-thiteration respectively.GFM Module This module mines the expectedtwo kinds of global features, based on which newsubjects andobjects features are generated. Thenthese two new generated features will be fed backtoTFG for next iteration. Speciﬁcally, this moduleconsists of following three steps.Step
.(2).TF(t)r(i, j) =WrReLU( H(t)s,i◦H(t)o,j) +br(2)where◦denotes the Hadamard Product operation,ReLU is the activation function, H(t)s,iandH(t)o,jarethe feature representations of tokens wiandwjatthet-thiteration respectively.GFM Module This module mines the expectedtwo kinds of global features, based on which newsubjects andobjects features are generated. Thenthese two new generated features will be fed backtoTFG for next iteration. Speciﬁcally, this moduleconsists of following three steps.Step 1 , to combine table features. Supposingcurrent iteration is t, we ﬁrst concatenate the ta-ble features of all relations together to generate anuniﬁed table feature (denoted as TF(t)). And thisuniﬁed table feature will contain the information ofboth token pairs and relations. Then we use a maxpooling operation and an FFN model on TF(t)togenerate a subject-related table feature ( TF(t)s) andan object-related table feature ( TF(t)o) respectively,as shown in Eq.(3).TF(t)s=Wsmaxpools(TF(t)) +bsTF(t)o
 1 , to combine table features. Supposingcurrent iteration is t, we ﬁrst concatenate the ta-ble features of all relations together to generate anuniﬁed table feature (denoted as TF(t)). And thisuniﬁed table feature will contain the information ofboth token pairs and relations. Then we use a maxpooling operation and an FFN model on TF(t)togenerate a subject-related table feature ( TF(t)s) andan object-related table feature ( TF(t)o) respectively,as shown in Eq.(3).TF(t)s=Wsmaxpools(TF(t)) +bsTF(t)o=Womaxpoolo(TF(t)) +bo(3)where Ws/o∈R(|L|×|R|)×dhare trainable weights,andbs/o∈Rdhare trainable biases.Here the max pooling is used to highlight theimportant features that are helpful for the subjectand object extractions respectively from a globalperspective.Step 2 , to mine the expected two kinds of globalfeatures. Here we mainly use a Transformer -basedmodel (Vaswani et al., 2017) to mine the globalassociations of relations and of token pairs.First, we use a Multi-Head Self-
=Womaxpoolo(TF(t)) +bo(3)where Ws/o∈R(|L|×|R|)×dhare trainable weights,andbs/o∈Rdhare trainable biases.Here the max pooling is used to highlight theimportant features that are helpful for the subjectand object extractions respectively from a globalperspective.Step 2 , to mine the expected two kinds of globalfeatures. Here we mainly use a Transformer -basedmodel (Vaswani et al., 2017) to mine the globalassociations of relations and of token pairs.First, we use a Multi-Head Self-Attention methodonTF(t)s/oto mine the global associations of rela-tions. The self-attention mechanism can reveal theimportance of an item from the perspective of otheritems, thus it is very suitable to mine the expectedrelation associations.Then we mine the global associations of tokenpairs with a Multi-Head Attention method. Thesentence representation His also taken as part ofinput here. We think Hmay contain some globalsemantic information of a token to some extentsince the input sentence is encoded as a whole,thus it is helpful for mining the global associationsof token pairs from a whole sentence perspective.Next, we generate
Attention methodonTF(t)s/oto mine the global associations of rela-tions. The self-attention mechanism can reveal theimportance of an item from the perspective of otheritems, thus it is very suitable to mine the expectedrelation associations.Then we mine the global associations of tokenpairs with a Multi-Head Attention method. Thesentence representation His also taken as part ofinput here. We think Hmay contain some globalsemantic information of a token to some extentsince the input sentence is encoded as a whole,thus it is helpful for mining the global associationsof token pairs from a whole sentence perspective.Next, we generate new subjects andobjects fea-tures with an FFN model.In summary, the whole global association miningprocess can be written with following Eq.(4).ˆTF(t)s/o= MultiHeadSelfAtt( TF(t)s/o)ˆH(t+1)(s/o)= MultiHeadAtt( ˆTF(t)s/o, H, H )H(t+1)(s/o)= ReLU( ˆH(t+1)(s/o)W+b)(4)Step 3 , to further tune the subjects andobjectsfeatures generated in previous step.One can notice that if we ﬂat the iterative mod-ules
 new subjects andobjects fea-tures with an FFN model.In summary, the whole global association miningprocess can be written with following Eq.(4).ˆTF(t)s/o= MultiHeadSelfAtt( TF(t)s/o)ˆH(t+1)(s/o)= MultiHeadAtt( ˆTF(t)s/o, H, H )H(t+1)(s/o)= ReLU( ˆH(t+1)(s/o)W+b)(4)Step 3 , to further tune the subjects andobjectsfeatures generated in previous step.One can notice that if we ﬂat the iterative mod-ules of TFG andGFM , our model would equal to avery deep network, thus it is possible to suffer fromthevanishing gradient issue. To avoid this, we usearesidual network to generate the ﬁnal subjectsandobjects features, as written in Eq. (5).H(t+1)(s/o)= LayerNorm ( H(t)(s/o)+H(t+1)(s/o))(5)Finally, these subjects andobjects features arefed back to the TFG module for next iteration. Notethat the parameters of TFG andGFM aresharedcross different iterations.TG Module Taking the table features
 of TFG andGFM , our model would equal to avery deep network, thus it is possible to suffer fromthevanishing gradient issue. To avoid this, we usearesidual network to generate the ﬁnal subjectsandobjects features, as written in Eq. (5).H(t+1)(s/o)= LayerNorm ( H(t)(s/o)+H(t+1)(s/o))(5)Finally, these subjects andobjects features arefed back to the TFG module for next iteration. Notethat the parameters of TFG andGFM aresharedcross different iterations.TG Module Taking the table features at the lastiteration ( TF(N)) as input, this module outputs allthe triples. Speciﬁcally, for each relation, its tableis ﬁrstly ﬁlled with the method shown in Eq. (6).ˆtable r(i, j) = softmax ( TF(N)r(i, j))table r(i, j) = argmaxl∈L(ˆtable r(i, j)[l])(6)where ˆtable r(i, j)∈R|L|, and table r(i, j)is thelabeled result for the token pair (wi, wj)in
 at the lastiteration ( TF(N)) as input, this module outputs allthe triples. Speciﬁcally, for each relation, its tableis ﬁrstly ﬁlled with the method shown in Eq. (6).ˆtable r(i, j) = softmax ( TF(N)r(i, j))table r(i, j) = argmaxl∈L(ˆtable r(i, j)[l])(6)where ˆtable r(i, j)∈R|L|, and table r(i, j)is thelabeled result for the token pair (wi, wj)in thetable of relation r.Then, TGdecodes the ﬁlled tables and deducesall triples with Algorithm 1. The main idea ofour algorithm is to generate an entity pair set foreach relation according to its ﬁlled table. And each Algorithm 1 Table Decoding StrategyInput: The relation set R, the sentence S= {w1,w2, ...,wn},and all table r∈Rn×nfor each relation r∈R.Output: The predicted triplet set, RT.1Deﬁne two temporary triple sets HandT, and initializeH, T, RT←∅,∅,∅.
 thetable of relation r.Then, TGdecodes the ﬁlled tables and deducesall triples with Algorithm 1. The main idea ofour algorithm is to generate an entity pair set foreach relation according to its ﬁlled table. And each Algorithm 1 Table Decoding StrategyInput: The relation set R, the sentence S= {w1,w2, ...,wn},and all table r∈Rn×nfor each relation r∈R.Output: The predicted triplet set, RT.1Deﬁne two temporary triple sets HandT, and initializeH, T, RT←∅,∅,∅.2foreachr∈Rdo3 Deﬁne three temporary sets WPHr,WPTr, andWPSr,which consist of token pairs whose ending tags intable rare "H", "T" and "S" respectively.4 foreach(wi, wj)∈WPHrdo // forward search5 1) Find a token pair (wk, wm)from WPTrthatsatisﬁes: i≤k,j≤m,table r[(wi, wj)]andtable r[(wk, wm)]match, (wi, wj)and(wk, wm)are closest in the table
2foreachr∈Rdo3 Deﬁne three temporary sets WPHr,WPTr, andWPSr,which consist of token pairs whose ending tags intable rare "H", "T" and "S" respectively.4 foreach(wi, wj)∈WPHrdo // forward search5 1) Find a token pair (wk, wm)from WPTrthatsatisﬁes: i≤k,j≤m,table r[(wi, wj)]andtable r[(wk, wm)]match, (wi, wj)and(wk, wm)are closest in the table, and the number of wordscontained in subject wi...k and object wj...m areconsistent with the corresponding tags.6 2) Add (wi...k, r, w j...m)toH.7 end for8 foreach(wk, wm)∈WPTrdo // reverse search9 1) Find a token pair (wi, wj)fromWPHrwith asimilar process as forward search.10 2) Add (wi...k, r, w j...m)toT.11 end for12 foreach(wi, wj)∈WPSrdo1
, and the number of wordscontained in subject wi...k and object wj...m areconsistent with the corresponding tags.6 2) Add (wi...k, r, w j...m)toH.7 end for8 foreach(wk, wm)∈WPTrdo // reverse search9 1) Find a token pair (wi, wj)fromWPHrwith asimilar process as forward search.10 2) Add (wi...k, r, w j...m)toT.11 end for12 foreach(wi, wj)∈WPSrdo13 Add (wi, r, w j)toRT14 end for15end for16RT←RT∪H∪T17return RTentity pair in this set would correspond to a minimalcontinuous token span in the ﬁlled table. Then eachentity pair would form a triple with the relation thatcorresponds to the considered table.Speciﬁcally, in our decoding algorithm, we de-sign three paralleled search routes to extract thetriples of each relation. The ﬁrst one ( forwardsearch , red arrows in Figure 1) generates triples inan order of from head to tail.
3 Add (wi, r, w j)toRT14 end for15end for16RT←RT∪H∪T17return RTentity pair in this set would correspond to a minimalcontinuous token span in the ﬁlled table. Then eachentity pair would form a triple with the relation thatcorresponds to the considered table.Speciﬁcally, in our decoding algorithm, we de-sign three paralleled search routes to extract thetriples of each relation. The ﬁrst one ( forwardsearch , red arrows in Figure 1) generates triples inan order of from head to tail. The second one ( re-verse search , green arrows in Figure 1) generatestriples in an order of from tail to head, which isdesigned mainly to handle the nested entities. Andthe third one (blue arrows in Figure 1) generatestriples that have single-token entity pairs.Here we take the sentence shown in Figure 1 asa concrete sample to further explain our decodingalgorithm. For example, in the demonstrated table,the token pair ( Edward ,New) has an "MMH" label,so the algorithm has to search forward to concate-nate adjacent token pairs until a token pair that hasa label "
 The second one ( re-verse search , green arrows in Figure 1) generatestriples in an order of from tail to head, which isdesigned mainly to handle the nested entities. Andthe third one (blue arrows in Figure 1) generatestriples that have single-token entity pairs.Here we take the sentence shown in Figure 1 asa concrete sample to further explain our decodingalgorithm. For example, in the demonstrated table,the token pair ( Edward ,New) has an "MMH" label,so the algorithm has to search forward to concate-nate adjacent token pairs until a token pair that hasa label "MMT" is found, so that to form the com-plete ( subject ,object ) pair. And the forward searchwould be stopped when it meets the token pair(Thomas ,York) that has the label "MMT". How-ever, the formed entity pair ( Edward Thomas ,NewYork) is a wrong entity pair in the demonstrated ex-ample since the expected pair is ( Edward Thomas ,CategoryNYT29 NYT24 WebNLGTrain Test Train Test Train TestNormal 53444 2963 37013 3266 1596 2
MMT" is found, so that to form the com-plete ( subject ,object ) pair. And the forward searchwould be stopped when it meets the token pair(Thomas ,York) that has the label "MMT". How-ever, the formed entity pair ( Edward Thomas ,NewYork) is a wrong entity pair in the demonstrated ex-ample since the expected pair is ( Edward Thomas ,CategoryNYT29 NYT24 WebNLGTrain Test Train Test Train TestNormal 53444 2963 37013 3266 1596 246EPO 8379 898 9782 978 227 26SEO 9862 1043 14735 1297 3406 457ALL 63306 4006 56195 5000 5019 703Relation 29 24 216 / 171∗Table 1: Statistics of datasets. EPO andSEO refer toentity pair overlapping andsingle
46EPO 8379 898 9782 978 227 26SEO 9862 1043 14735 1297 3406 457ALL 63306 4006 56195 5000 5019 703Relation 29 24 216 / 171∗Table 1: Statistics of datasets. EPO andSEO refer toentity pair overlapping andsingle entity overlappingrespectively (Zeng et al., 2018). Note that a sentencecan belong to both EPO andSEO. And 216 / 171∗means that there are 216 / 171 relations in WebNLGand WebNLG∗respectively.New York City ). Such kind of errors are caused bythe nested entities in the input sentence, like the“New York ” and “ New York City ”. These nested en-tities will make the forward search stops too early.In such case, the designed reverse search will playan important supplementary role. In the discussed
 entity overlappingrespectively (Zeng et al., 2018). Note that a sentencecan belong to both EPO andSEO. And 216 / 171∗means that there are 216 / 171 relations in WebNLGand WebNLG∗respectively.New York City ). Such kind of errors are caused bythe nested entities in the input sentence, like the“New York ” and “ New York City ”. These nested en-tities will make the forward search stops too early.In such case, the designed reverse search will playan important supplementary role. In the discussedexample, the reverse search will ﬁrst ﬁnd the tokenpair ( Thomas ,City) that has an "MMT" label andhas to further ﬁnd a token pair that has an "MMH"label. Thus it will precisely ﬁnd the expected entitypair ( Edward Thomas ,New York City ).Of course, if there are few nested entities in adataset, the reverse search can be removed, whichwould be better for the running time. Here we leaveit to make our model have a better generalizationability so that can be used in diverse datasets.3.3 Loss FunctionWe deﬁne the model
example, the reverse search will ﬁrst ﬁnd the tokenpair ( Thomas ,City) that has an "MMT" label andhas to further ﬁnd a token pair that has an "MMH"label. Thus it will precisely ﬁnd the expected entitypair ( Edward Thomas ,New York City ).Of course, if there are few nested entities in adataset, the reverse search can be removed, whichwould be better for the running time. Here we leaveit to make our model have a better generalizationability so that can be used in diverse datasets.3.3 Loss FunctionWe deﬁne the model loss as follows.L=n∑i=1n∑j=1|R|∑r=1−logp(yr,(i,j)=table r(i, j))=n∑i=1n∑j=1|R|∑r=1−log ˆtable r(i, j)[yr,(i,j)](7)where yr,(i,j)∈[1,|L|]is the index of the groundtruth label of (wi, wj)for the relaion r.4 Experiments4.1 Experimental SettingsDatasets We evaluate our model on three bench-mark datasets: NYT2
 loss as follows.L=n∑i=1n∑j=1|R|∑r=1−logp(yr,(i,j)=table r(i, j))=n∑i=1n∑j=1|R|∑r=1−log ˆtable r(i, j)[yr,(i,j)](7)where yr,(i,j)∈[1,|L|]is the index of the groundtruth label of (wi, wj)for the relaion r.4 Experiments4.1 Experimental SettingsDatasets We evaluate our model on three bench-mark datasets: NYT29 (Takanobu et al., 2019),NYT24 (Zeng et al., 2018) and WebNLG (Gardentet al., 2017). Both NYT24 and WebNLG have twodifferent versions according to following two an-notation standards: 1) annotating the last token of each entity, and 2) annotating the whole entity span.Different work chooses different versions of thesedatasets. To evaluate our model comprehensively,we use both kinds of datasets. For convenience,we denote the datasets based on the �
9 (Takanobu et al., 2019),NYT24 (Zeng et al., 2018) and WebNLG (Gardentet al., 2017). Both NYT24 and WebNLG have twodifferent versions according to following two an-notation standards: 1) annotating the last token of each entity, and 2) annotating the whole entity span.Different work chooses different versions of thesedatasets. To evaluate our model comprehensively,we use both kinds of datasets. For convenience,we denote the datasets based on the ﬁrst annota-tion standard as NYT24∗and WebNLG∗, and thedatasets based on the second annotation standardas NYT24 and WebNLG. Some statistics of thesedatasets are shown in Table 1.Evaluation Metrics The standard micro precision,recall, and F1score are used to evaluate the results.Note that there are two match standards for theRTE task: one is Partial Match that an extractedtriplet is regarded as correct if the predicted relationand the head tokens of both subject entity and ob-ject entity are correct; and the other is Exact Matchthat a triple would be considered correct only
��rst annota-tion standard as NYT24∗and WebNLG∗, and thedatasets based on the second annotation standardas NYT24 and WebNLG. Some statistics of thesedatasets are shown in Table 1.Evaluation Metrics The standard micro precision,recall, and F1score are used to evaluate the results.Note that there are two match standards for theRTE task: one is Partial Match that an extractedtriplet is regarded as correct if the predicted relationand the head tokens of both subject entity and ob-ject entity are correct; and the other is Exact Matchthat a triple would be considered correct only whenits entities and relation are completely matchedwith a correct triple. To fairly compare with exist-ing models, we follow previous work (Wang et al.,2020; Wei et al., 2020; Sun et al., 2021) and usePartial Match on NYT24∗and WebNLG∗, and useExact Match on NYT24, NYT29, and WebNLG.In fact, since only one token of each entity inNYT24∗and WebNLG∗is annotated, the resultsofPartial Match andExact Match on these twodatasets are actually
 whenits entities and relation are completely matchedwith a correct triple. To fairly compare with exist-ing models, we follow previous work (Wang et al.,2020; Wei et al., 2020; Sun et al., 2021) and usePartial Match on NYT24∗and WebNLG∗, and useExact Match on NYT24, NYT29, and WebNLG.In fact, since only one token of each entity inNYT24∗and WebNLG∗is annotated, the resultsofPartial Match andExact Match on these twodatasets are actually the same.Baselines We compare our model with followingstrong state-of-the-art RTE models: CopyRE (Zenget al., 2018), GraphRel (Fu et al., 2019), Copy-MTL (Zeng et al., 2020), OrderCopyRE (Zeng et al.,2019), ETL-Span (Yu et al., 2019), WDec (Nayakand Ng, 2020), RSAN (Yuan et al., 2020),RIN (Sun et al., 20
 the same.Baselines We compare our model with followingstrong state-of-the-art RTE models: CopyRE (Zenget al., 2018), GraphRel (Fu et al., 2019), Copy-MTL (Zeng et al., 2020), OrderCopyRE (Zeng et al.,2019), ETL-Span (Yu et al., 2019), WDec (Nayakand Ng, 2020), RSAN (Yuan et al., 2020),RIN (Sun et al., 2020), CasRel (Wei et al., 2020),TPLinker (Wang et al., 2020), SPN (Sui et al.,2020), and PMEI (Sun et al., 2021).Most of the experimental results of these base-lines are copied from their original papers directly.Some baselines did not report their results on someof the used datasets. In such case, we report thebest results we obtained with the provided sourcecode (if the source codes is available). For simplic-ity, we denote our model as GRTE , the abbreviation
20), CasRel (Wei et al., 2020),TPLinker (Wang et al., 2020), SPN (Sui et al.,2020), and PMEI (Sun et al., 2021).Most of the experimental results of these base-lines are copied from their original papers directly.Some baselines did not report their results on someof the used datasets. In such case, we report thebest results we obtained with the provided sourcecode (if the source codes is available). For simplic-ity, we denote our model as GRTE , the abbreviationofGlobal feature oriented RTE model.Implementation Details Adam (Kingma and Ba,2015) is used to optimize GRTE . The learning rate,epoch and batch size are set to 3 ×10−5,50,6re-spectively. The iteration numbers (the hyperparam-eterN) on NYT29, NYT24∗, NYT24, WebNLG∗and WebNLG are set to 3, 2, 3, 2, and 4 respec-tively. Following previous work (Wei et al., 2020;Sun et
ofGlobal feature oriented RTE model.Implementation Details Adam (Kingma and Ba,2015) is used to optimize GRTE . The learning rate,epoch and batch size are set to 3 ×10−5,50,6re-spectively. The iteration numbers (the hyperparam-eterN) on NYT29, NYT24∗, NYT24, WebNLG∗and WebNLG are set to 3, 2, 3, 2, and 4 respec-tively. Following previous work (Wei et al., 2020;Sun et al., 2021; Wang et al., 2020), we also imple-ment a BiLSTM -encoder version of GRTE where300-dimensional GloVe embeddings (Penningtonet al., 2014) and 2-layer stacked BiLSTM are used.In this version, the hidden dimension of these 2layers are set as 300 and 600 respectively. All thehyperparameters reported in this work are deter-mined based on the results on the development sets.Other parameters are randomly initialized. Follow-ingCasRel andTPLinker , the max length
 al., 2021; Wang et al., 2020), we also imple-ment a BiLSTM -encoder version of GRTE where300-dimensional GloVe embeddings (Penningtonet al., 2014) and 2-layer stacked BiLSTM are used.In this version, the hidden dimension of these 2layers are set as 300 and 600 respectively. All thehyperparameters reported in this work are deter-mined based on the results on the development sets.Other parameters are randomly initialized. Follow-ingCasRel andTPLinker , the max length of inputsentences is set to 100.4.2 Main Experimental ResultsThe main results are in the top two parts of Table 2,which show GRTE is very effective. On all datasets,it achieves almost all the best results in term of F1compared with the models that use the same kindof encoder (either the BiLSTM based encoder orthe BERT based encoder). The only exception ison NYT24∗, where the F1ofGRTE LSTM is about1% lower than that of PMEI LSTM . However, onthe same dataset, the F1score of GRTE BERT
 of inputsentences is set to 100.4.2 Main Experimental ResultsThe main results are in the top two parts of Table 2,which show GRTE is very effective. On all datasets,it achieves almost all the best results in term of F1compared with the models that use the same kindof encoder (either the BiLSTM based encoder orthe BERT based encoder). The only exception ison NYT24∗, where the F1ofGRTE LSTM is about1% lower than that of PMEI LSTM . However, onthe same dataset, the F1score of GRTE BERT isabout 2.9% higher than that of PMEI BERT .The results also show that GRTE achieves muchbetter results on NYT29, NYT24 and WebNLG:itsF1scores improve about 1.9%, 1.1%, and3.3% over the previous best models on these threedatasets respectively. Contrastively, its F1scoresimprove about 0.5% and 0.5% over the previousbest models on NYT24∗and WebNLG∗respec-tively. This is mainly because that GRTE could notrealize its full potential
 isabout 2.9% higher than that of PMEI BERT .The results also show that GRTE achieves muchbetter results on NYT29, NYT24 and WebNLG:itsF1scores improve about 1.9%, 1.1%, and3.3% over the previous best models on these threedatasets respectively. Contrastively, its F1scoresimprove about 0.5% and 0.5% over the previousbest models on NYT24∗and WebNLG∗respec-tively. This is mainly because that GRTE could notrealize its full potential on NYT24∗and WebNLG∗where only one token of each entity is annotated.For example, under this annotation standard, except"N/A", "SSH", and "SST", all the other deﬁnedlabels in GRTE are redundant. But it should benoted that the annotation standard on NYT24∗andWebNLG∗simpliﬁes the RTE task, there wouldnot be such a standard when a model is really de-ployed. Thus, the annotation standard on NYT29,NYT24 and WebNLG can better reveal the true per-formance of a model. Accordingly,
 on NYT24∗and WebNLG∗where only one token of each entity is annotated.For example, under this annotation standard, except"N/A", "SSH", and "SST", all the other deﬁnedlabels in GRTE are redundant. But it should benoted that the annotation standard on NYT24∗andWebNLG∗simpliﬁes the RTE task, there wouldnot be such a standard when a model is really de-ployed. Thus, the annotation standard on NYT29,NYT24 and WebNLG can better reveal the true per-formance of a model. Accordingly, GRTE ’s betterperformance on them is more meaningful.We can further see that compared with the pre-vious best models, GRTE achieves more perfor-mance improvement on WebNLG than on otherdatasets. For example, GRTE LSTM even outper-forms all other compared baselines on WebNLG, in-cluding those models that use BERT . We think thisis mainly because that the numbers of relations inWebNLG are far more than those of in NYT29 and ModelNYT29 NYT24⋆NYT24 WebNLG⋆WebNLGPrec. Rec. F1 Prec. Rec
 GRTE ’s betterperformance on them is more meaningful.We can further see that compared with the pre-vious best models, GRTE achieves more perfor-mance improvement on WebNLG than on otherdatasets. For example, GRTE LSTM even outper-forms all other compared baselines on WebNLG, in-cluding those models that use BERT . We think thisis mainly because that the numbers of relations inWebNLG are far more than those of in NYT29 and ModelNYT29 NYT24⋆NYT24 WebNLG⋆WebNLGPrec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1CopyRE – – – 61.0 56.6 58.7 – – – 37.7 36.4 37.1 – – –GraphRel – – – 63.9 60.0 61.9 – – – 44.7 41.1 42.9 – – –OrderCopyRE – – – 77.9 67.2 72.1 – – – 
. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1CopyRE – – – 61.0 56.6 58.7 – – – 37.7 36.4 37.1 – – –GraphRel – – – 63.9 60.0 61.9 – – – 44.7 41.1 42.9 – – –OrderCopyRE – – – 77.9 67.2 72.1 – – – 63.3 59.9 61.6 – – –ETL-Span 74.5⋆57.9⋆65.2⋆84.9 72.3 78.1 85.5 71.7 78.0 84.0 91.5 87.6 84.3 82.0 83.1WDec 77.7 60.8 68.2 – – – 88.1 76.1 8
63.3 59.9 61.6 – – –ETL-Span 74.5⋆57.9⋆65.2⋆84.9 72.3 78.1 85.5 71.7 78.0 84.0 91.5 87.6 84.3 82.0 83.1WDec 77.7 60.8 68.2 – – – 88.1 76.1 81.7 – – – – – –RSAN – – – – – – 85.7 83.6 84.6 – – – 80.5 83.8 82.1RIN – – – 87.2 87.3 87.3 83.9 85.5 84.7 87.6 87.0 87.3 77.3 76.8 77.0CasRel LSTM – – – 84.2 8
1.7 – – – – – –RSAN – – – – – – 85.7 83.6 84.6 – – – 80.5 83.8 82.1RIN – – – 87.2 87.3 87.3 83.9 85.5 84.7 87.6 87.0 87.3 77.3 76.8 77.0CasRel LSTM – – – 84.2 83.0 83.6 – – – 86.9 80.6 83.7 – – –PMEI LSTM – – – 88.7 86.8 87.8 84.5 84.0 84.2 88.7 87.6 88.1 78.8 77.7 78.2TPLinker LSTM – – – 83.8 83.4 83.6 86.0 82.0 
3.0 83.6 – – – 86.9 80.6 83.7 – – –PMEI LSTM – – – 88.7 86.8 87.8 84.5 84.0 84.2 88.7 87.6 88.1 78.8 77.7 78.2TPLinker LSTM – – – 83.8 83.4 83.6 86.0 82.0 84.0 90.8 90.3 90.5 91.9 81.6 86.4CasRel BERT 77.0⋆68.0⋆72.2⋆89.7 89.5 89.6 89.8⋆88.2⋆89.0⋆93.4 90.1 91.8 88.3⋆84.6⋆86.4⋆PMEI BERT – – – 90.5 
84.0 90.8 90.3 90.5 91.9 81.6 86.4CasRel BERT 77.0⋆68.0⋆72.2⋆89.7 89.5 89.6 89.8⋆88.2⋆89.0⋆93.4 90.1 91.8 88.3⋆84.6⋆86.4⋆PMEI BERT – – – 90.5 89.8 90.1 88.4 88.9 88.7 91.0 92.9 92.0 80.8 82.8 81.8TPLinker BERT 78.0∗68.1∗72.7∗91.3 92.5 91.9 91.4 92.6 92.0 91.8 92.0 91.9 88.9 84.5
89.8 90.1 88.4 88.9 88.7 91.0 92.9 92.0 80.8 82.8 81.8TPLinker BERT 78.0∗68.1∗72.7∗91.3 92.5 91.9 91.4 92.6 92.0 91.8 92.0 91.9 88.9 84.5 86.7SPNBERT 76.0∗71.0∗73.4∗93.3 91.7 92.5 92.5 92.2 92.3 93.1 93.6 93.4 85.7⋆82.9⋆84.3⋆GRTE LSTM 74.3 67.9 71.0 87.5 86.1 86.8 86.2 87.1 
 86.7SPNBERT 76.0∗71.0∗73.4∗93.3 91.7 92.5 92.5 92.2 92.3 93.1 93.6 93.4 85.7⋆82.9⋆84.3⋆GRTE LSTM 74.3 67.9 71.0 87.5 86.1 86.8 86.2 87.1 86.6 90.1 91.6 90.8 88.0 86.3 87.1GRTE BERT 80.1 71.0 75.3 92.9 93.1 93.0 93.4 93.5 93.4 93.7 94.2 93.9 92.3 87.9 90.0GRTE w/o GFM 77.9 68.
86.6 90.1 91.6 90.8 88.0 86.3 87.1GRTE BERT 80.1 71.0 75.3 92.9 93.1 93.0 93.4 93.5 93.4 93.7 94.2 93.9 92.3 87.9 90.0GRTE w/o GFM 77.9 68.9 73.1 90.6 92.5 91.5 91.8 92.6 92.2 92.4 91.1 91.7 88.4 86.7 87.5GRTE GRU GFM 78.2 71.7 74.8 92.5 92.9 92.7 93.4 92.2 92.8 93.4 92.6 
9 73.1 90.6 92.5 91.5 91.8 92.6 92.2 92.4 91.1 91.7 88.4 86.7 87.5GRTE GRU GFM 78.2 71.7 74.8 92.5 92.9 92.7 93.4 92.2 92.8 93.4 92.6 93.0 90.1 88.0 89.0GRTE w/o m−h 77.8 70.9 74.2 91.9 92.9 92.4 93.2 92.9 93.0 92.9 92.1 92.5 90.5 87.6 89.0GRTE w/o shared 79.5 71.5 75.3 92.7 9
93.0 90.1 88.0 89.0GRTE w/o m−h 77.8 70.9 74.2 91.9 92.9 92.4 93.2 92.9 93.0 92.9 92.1 92.5 90.5 87.6 89.0GRTE w/o shared 79.5 71.5 75.3 92.7 93.0 92.8 93.6 92.7 93.1 93.4 94.0 93.7 91.5 87.4 89.4Table 2: Main results. A model with a subscript LSTM refers to replace its BERT based encoder with the BiLSTMbased encoder.⋆means the results are produced by us with the available source code.NYT24 (see Table 1), which means there are moreglobal associations of relations can be mined. Gen-erally, the more relations and entities
3.0 92.8 93.6 92.7 93.1 93.4 94.0 93.7 91.5 87.4 89.4Table 2: Main results. A model with a subscript LSTM refers to replace its BERT based encoder with the BiLSTMbased encoder.⋆means the results are produced by us with the available source code.NYT24 (see Table 1), which means there are moreglobal associations of relations can be mined. Gen-erally, the more relations and entities there are in adataset, the more global correlations there wouldbe among triples. Accordingly, our model couldperform more better on such kind of datasets thanother local features based methods. For example,the number of relations in WebNLG is almost 7times of those in NYT, and GRTE achieves muchmore performance improvement over the comparedbaselines on WebNLG than on NYT.4.3 Detailed ResultsIn this section, we conduct detailed experiments todemonstrate the effectiveness of our model fromfollowing two aspects.First , we conduct some ablation experiments toevaluate the contributions of some main compo-nents in GRTE . To this end
 there are in adataset, the more global correlations there wouldbe among triples. Accordingly, our model couldperform more better on such kind of datasets thanother local features based methods. For example,the number of relations in WebNLG is almost 7times of those in NYT, and GRTE achieves muchmore performance improvement over the comparedbaselines on WebNLG than on NYT.4.3 Detailed ResultsIn this section, we conduct detailed experiments todemonstrate the effectiveness of our model fromfollowing two aspects.First , we conduct some ablation experiments toevaluate the contributions of some main compo-nents in GRTE . To this end, we implement follow-ing model variants.(i) GRTE w/o GFM , a variant that removes theGFM module completely from GRTE , which is toevaluate the contribution of GFM . Like previous ta-ble ﬁlling based methods, GRTE w/o GFM extractstriples only based on local features.(ii) GRTE GRU GIF , a variant that uses GRU (tak-ingHandTF(t)s/oas input) instead of Transformerto generate the results in Eq. (4), which is to evalu-ate the contribution of Transformer .(iii) GRTE w/o m−h,
, we implement follow-ing model variants.(i) GRTE w/o GFM , a variant that removes theGFM module completely from GRTE , which is toevaluate the contribution of GFM . Like previous ta-ble ﬁlling based methods, GRTE w/o GFM extractstriples only based on local features.(ii) GRTE GRU GIF , a variant that uses GRU (tak-ingHandTF(t)s/oas input) instead of Transformerto generate the results in Eq. (4), which is to evalu-ate the contribution of Transformer .(iii) GRTE w/o m−h, a variant that replaces themulti-head attention method in GFM with a single-head attention method, which is to evaluate thecontribution of the multi-head attention .(iv) GRTE w/o shared , a variant that uses differentparameters for the modules of TFG andGFM atdifferent iterations, which is to evaluate the contri-bution of the parameter share mechanism.All these variants use the BERT -based encoder.And their results are shown in the bottom part ofTable 2, from which we can make following obser-vations.(1) The performance of GRTE w/o GFM dropsgreatly compared with GRTE
 a variant that replaces themulti-head attention method in GFM with a single-head attention method, which is to evaluate thecontribution of the multi-head attention .(iv) GRTE w/o shared , a variant that uses differentparameters for the modules of TFG andGFM atdifferent iterations, which is to evaluate the contri-bution of the parameter share mechanism.All these variants use the BERT -based encoder.And their results are shown in the bottom part ofTable 2, from which we can make following obser-vations.(1) The performance of GRTE w/o GFM dropsgreatly compared with GRTE , which conﬁrms theimportance of using two kinds of global featuresfor table ﬁlling. We can further notice that onNYT29, NYT24, and WebNLG, the F1scoresof GRTE w/o GFM increases by 0.4%, 0.4%, and0.8% respectively over TPLinker . Both TPLinkerand GRTE w/o GFM extract triples based on lo-cal features, and the main difference betweenthem is the table ﬁlling strategy. So these resultsprove the effectiveness of our table ﬁlling strat-eg
 , which conﬁrms theimportance of using two kinds of global featuresfor table ﬁlling. We can further notice that onNYT29, NYT24, and WebNLG, the F1scoresof GRTE w/o GFM increases by 0.4%, 0.4%, and0.8% respectively over TPLinker . Both TPLinkerand GRTE w/o GFM extract triples based on lo-cal features, and the main difference betweenthem is the table ﬁlling strategy. So these resultsprove the effectiveness of our table ﬁlling strat-egy. The F1scores of GRTE w/o GFM on NYT24∗and WebNLG∗are slightly lower than those ofTPLinker , as explained above, this is because eachentity in NYT24∗and WebNLG∗, only one tokenis annotated for each entity, GRTE w/o GFM couldnot realize its full potential. ModelNYT24⋆WebNLG⋆Normal SEO EPO T = 1 T = 2 T = 3 T = 4 T ≥5 Normal SEO EPO T = 1 T = 2 T = 3 T = 4 T
y. The F1scores of GRTE w/o GFM on NYT24∗and WebNLG∗are slightly lower than those ofTPLinker , as explained above, this is because eachentity in NYT24∗and WebNLG∗, only one tokenis annotated for each entity, GRTE w/o GFM couldnot realize its full potential. ModelNYT24⋆WebNLG⋆Normal SEO EPO T = 1 T = 2 T = 3 T = 4 T ≥5 Normal SEO EPO T = 1 T = 2 T = 3 T = 4 T ≥5CasRel BERT 87.3 91.4 92 88.2 90.3 91.9 94.2 83.7 89.4 92.2 94.7 89.3 90.8 94.2 92.4 90.9TPLinker BERT 90.1 93.4 94.0 90.0 92.8 93.1 96.1 90.0
 ≥5CasRel BERT 87.3 91.4 92 88.2 90.3 91.9 94.2 83.7 89.4 92.2 94.7 89.3 90.8 94.2 92.4 90.9TPLinker BERT 90.1 93.4 94.0 90.0 92.8 93.1 96.1 90.0 87.9 92.5 95.3 88.0 90.1 94.6 93.3 91.6SPNBERT 90.8 94.0 94.1 90.9 93.4 94.2 95.5 90.6 89.5∗94.1∗90.8∗89.5 91.3 96.4 94.7 93.8GRTE BERT 
 87.9 92.5 95.3 88.0 90.1 94.6 93.3 91.6SPNBERT 90.8 94.0 94.1 90.9 93.4 94.2 95.5 90.6 89.5∗94.1∗90.8∗89.5 91.3 96.4 94.7 93.8GRTE BERT 91.1 94.4 95 90.8 93.7 94.4 96.2 93.4 90.6 94.5 96 90.6 92.5 96.5 95.5 94.4Table 3: F1 scores on sentences with different overlapping pattern and different triplet number. Results of CasRelare copied from TPLinker directly. “T” is the number of triples contained in a sentence.∗means the results areproduced by us with
91.1 94.4 95 90.8 93.7 94.4 96.2 93.4 90.6 94.5 96 90.6 92.5 96.5 95.5 94.4Table 3: F1 scores on sentences with different overlapping pattern and different triplet number. Results of CasRelare copied from TPLinker directly. “T” is the number of triples contained in a sentence.∗means the results areproduced by us with the provided source codes.Figure 3: F1 results under different N.(2) The performance of GRTE GRU GFM dropscompared with GRTE , which indicates Trans-former is more suitable for the global feature min-ing than GRU . But even so, we can see that on alldatasets, GRTE GRU GFM outperforms almost allprevious best models and GRTE w/o GFM in termofF1, which further indicates the effectiveness ofusing global features.(3) The results of GRTE w/o m−hare lower thanthose of GRTE , which shows the multi-head atten
 the provided source codes.Figure 3: F1 results under different N.(2) The performance of GRTE GRU GFM dropscompared with GRTE , which indicates Trans-former is more suitable for the global feature min-ing than GRU . But even so, we can see that on alldatasets, GRTE GRU GFM outperforms almost allprevious best models and GRTE w/o GFM in termofF1, which further indicates the effectiveness ofusing global features.(3) The results of GRTE w/o m−hare lower thanthose of GRTE , which shows the multi-head atten-tionmechanism plays an important role for globalfeature mining. In fact, the importance of differ-ent features is different, the multi-head attentionmechanism performs the feature mining processfrom multiple aspects, which is much helpful tohighlight the more important ones.(4) The results of GRTE w/o shared are slightlylower than those of GRTE , which shows the sharemechanism is effective. In fact, the mechanismof using distinct parameters usually works wellonly when the training samples are sufﬁcient. Butthis condition is not well satisﬁed in RTE sincethe training samples of a dataset are not sufﬁ
-tionmechanism plays an important role for globalfeature mining. In fact, the importance of differ-ent features is different, the multi-head attentionmechanism performs the feature mining processfrom multiple aspects, which is much helpful tohighlight the more important ones.(4) The results of GRTE w/o shared are slightlylower than those of GRTE , which shows the sharemechanism is effective. In fact, the mechanismof using distinct parameters usually works wellonly when the training samples are sufﬁcient. Butthis condition is not well satisﬁed in RTE sincethe training samples of a dataset are not sufﬁcientenough to train too many parameters.Second , we evaluate the inﬂuence of the itera-tion number N. The results are shown in Figure 3,from which following observations can be made.(1) On NYT24∗and WebNLG∗, the annotationstandard is relatively simple. So GRTE achievesthe best results with two iterations. But on NYT29,NYT24, and WebNLG, more iterations are usuallyrequired. For example, GRTE achieves the bestresults when Nis 3, 3, and 4 respectively on them.(2) On all datasets, GR
cientenough to train too many parameters.Second , we evaluate the inﬂuence of the itera-tion number N. The results are shown in Figure 3,from which following observations can be made.(1) On NYT24∗and WebNLG∗, the annotationstandard is relatively simple. So GRTE achievesthe best results with two iterations. But on NYT29,NYT24, and WebNLG, more iterations are usuallyrequired. For example, GRTE achieves the bestresults when Nis 3, 3, and 4 respectively on them.(2) On all datasets, GRTE gets obvious perfor-mance improvement (even the maximum perfor-mance improvement on some datasets) at N= 2where GFM begins to play its role , which indicatesagain that using global features can signiﬁcantlyimprove the model performance.(3)GRTE usually achieves the best results withina small number of iterations on all datasets includ-ing WebNLG or WebNLG∗where there are lots ofrelations. In fact, GRTE outperforms all the pervi-ous best models even when N= 2. This is a veryimportant merit because it indicates that even usedon some datasets where the numbers
TE gets obvious perfor-mance improvement (even the maximum perfor-mance improvement on some datasets) at N= 2where GFM begins to play its role , which indicatesagain that using global features can signiﬁcantlyimprove the model performance.(3)GRTE usually achieves the best results withina small number of iterations on all datasets includ-ing WebNLG or WebNLG∗where there are lots ofrelations. In fact, GRTE outperforms all the pervi-ous best models even when N= 2. This is a veryimportant merit because it indicates that even usedon some datasets where the numbers of relationsare very large, the efﬁciency would not be a burdenforGRTE , which is much meaningful when GRTEis deployed in some real scenarios.4.4 Analysis on Different Sentence TypesHere we evaluate GRTE ’s ability for extractingtriples from sentences that contain overlappingtriples and multiple triples. For fair comparisonwith the previous best models ( CasRel ,TPLinker ,andSPN), we follow their settings which are: (i)classifying sentences according to the degree ofoverlapping and the number of triples containedin a sentence, and (ii) conducting experiments ondifferent subsets of NYT24∗
 of relationsare very large, the efﬁciency would not be a burdenforGRTE , which is much meaningful when GRTEis deployed in some real scenarios.4.4 Analysis on Different Sentence TypesHere we evaluate GRTE ’s ability for extractingtriples from sentences that contain overlappingtriples and multiple triples. For fair comparisonwith the previous best models ( CasRel ,TPLinker ,andSPN), we follow their settings which are: (i)classifying sentences according to the degree ofoverlapping and the number of triples containedin a sentence, and (ii) conducting experiments ondifferent subsets of NYT24∗and WebNLG∗.The results are shown in Table 3. We can seethat: (i) GRTE achieves the best results on all threekinds of overlapping sentences on both datasets,and (ii) GRTE achieves the best results on almostall kinds of sentences that contain multiple triples.The only exception is on NYT24∗where the F1score of GRTE is slightly lower than that of SPNwhen Tis 1. The main reason is that there are lessassociations among token pairs when Tis 1, whichslightly degrades the performance of GRTE . ModelNYT24⋆
and WebNLG∗.The results are shown in Table 3. We can seethat: (i) GRTE achieves the best results on all threekinds of overlapping sentences on both datasets,and (ii) GRTE achieves the best results on almostall kinds of sentences that contain multiple triples.The only exception is on NYT24∗where the F1score of GRTE is slightly lower than that of SPNwhen Tis 1. The main reason is that there are lessassociations among token pairs when Tis 1, whichslightly degrades the performance of GRTE . ModelNYT24⋆WebNLG⋆Params all Prop encoder Inference Time Params all Prop encoder Inference TimeCasRel BERT 107,719,680 99.96% 53.9 107,984,216 99.76% 77.5TPLinker BERT 109,602,962 98.82% 18.1 / 83.5†110,281,220 98.21% 26.9
WebNLG⋆Params all Prop encoder Inference Time Params all Prop encoder Inference TimeCasRel BERT 107,719,680 99.96% 53.9 107,984,216 99.76% 77.5TPLinker BERT 109,602,962 98.82% 18.1 / 83.5†110,281,220 98.21% 26.9 / 120.4†SPNBERT 141,428,765 76.58% 26.4 / 107.9†150,989,744 71.73% 22.6 / 105.7†GRTE BERT 119,387,328 90.72% 21.3 / 109.6†122,098,008 88.70%
 / 120.4†SPNBERT 141,428,765 76.58% 26.4 / 107.9†150,989,744 71.73% 22.6 / 105.7†GRTE BERT 119,387,328 90.72% 21.3 / 109.6†122,098,008 88.70% 28.7 / 124.1†Table 4: Computational efﬁciency. Params allis the number of parameters for the entire model. Prop encoder refersto the proportion of encoder parameters in the total model parameters. Inference Time represents the average time(millisecond) the model takes to process a sample.†marks the inference time when the batch size is set to 1.In fact, GRTE maintains a table for each relation,and the TGmodule extracts triples for each relationindependently. Thus it can well handle above twokinds of complex sentences by nature.4.5 Analysis on Computational Ef
 28.7 / 124.1†Table 4: Computational efﬁciency. Params allis the number of parameters for the entire model. Prop encoder refersto the proportion of encoder parameters in the total model parameters. Inference Time represents the average time(millisecond) the model takes to process a sample.†marks the inference time when the batch size is set to 1.In fact, GRTE maintains a table for each relation,and the TGmodule extracts triples for each relationindependently. Thus it can well handle above twokinds of complex sentences by nature.4.5 Analysis on Computational EfﬁciencyTable 4 shows the comparison results of computa-tional efﬁciency between GRTE and some previousbest models. To be fair, we follow the settingsinTPLinker : analyze the parameter scale and theinference time on NYT∗and WebNLG∗. All the re-sults are obtained by running the compared modelson a TitanXP, and the batch size is set to 6 for allmodels that can be run in a batch mode.The parameter number of GRTE is slightly largerthan that of TPLinker , which is mainly due to the us-ing of a Transformer -based model. But
ﬁciencyTable 4 shows the comparison results of computa-tional efﬁciency between GRTE and some previousbest models. To be fair, we follow the settingsinTPLinker : analyze the parameter scale and theinference time on NYT∗and WebNLG∗. All the re-sults are obtained by running the compared modelson a TitanXP, and the batch size is set to 6 for allmodels that can be run in a batch mode.The parameter number of GRTE is slightly largerthan that of TPLinker , which is mainly due to the us-ing of a Transformer -based model. But when com-pared with SPN that uses the Transformer modeltoo, we can see that GRTE has a smaller number ofparameters due to its parameter share mechanism.We can also see that GRTE achieves a very com-petitive inference speed. This is mainly becauseof following three reasons. First, GRTE is a one-stage extraction model and can process samples ina batch mode ( CasRel can only process samplesone by one). Second, as analyzed previously, it hasan efﬁcient table ﬁlling strategy that needs to ﬁllfewer table items. Third, as analyzed previously,GRTE often achieves the best
 when com-pared with SPN that uses the Transformer modeltoo, we can see that GRTE has a smaller number ofparameters due to its parameter share mechanism.We can also see that GRTE achieves a very com-petitive inference speed. This is mainly becauseof following three reasons. First, GRTE is a one-stage extraction model and can process samples ina batch mode ( CasRel can only process samplesone by one). Second, as analyzed previously, it hasan efﬁcient table ﬁlling strategy that needs to ﬁllfewer table items. Third, as analyzed previously,GRTE often achieves the best results within a smallnumber of iterations, thus the iteration operationswill not have too much impact on the inferencespeed of GRTE .In fact, as TPLinker pointed out that for all themodels that use BERT (or other kinds of pre-trainedlanguage models) as their basic encoders, BERT isusually the most time-consuming part and takes upthe most of model parameters, so the time cost ofother components in a model is not signiﬁcant.Besides, there is another important merit of ourmodel: it needs less training time than existingstate-of-the-art models like CasRel ,TPLinker ,
 results within a smallnumber of iterations, thus the iteration operationswill not have too much impact on the inferencespeed of GRTE .In fact, as TPLinker pointed out that for all themodels that use BERT (or other kinds of pre-trainedlanguage models) as their basic encoders, BERT isusually the most time-consuming part and takes upthe most of model parameters, so the time cost ofother components in a model is not signiﬁcant.Besides, there is another important merit of ourmodel: it needs less training time than existingstate-of-the-art models like CasRel ,TPLinker , andSPN etc. As pointed out previously, the epoch ofour model on all datasets is 50. But on the samedatasets, the epochs of all the mentioned modelsare 100. From Table 4 we can see that all thesemodels have similar inference speed. For eachmodel, the training speed of each epoch is veryclose to its inference speed (during training, therewould be extra time cost for operations like theback propagation), thus we can easily know thatour model needs less time for training since ourmodel has a far less epoch number.5 ConclusionsIn this study, we propose a novel
 andSPN etc. As pointed out previously, the epoch ofour model on all datasets is 50. But on the samedatasets, the epochs of all the mentioned modelsare 100. From Table 4 we can see that all thesemodels have similar inference speed. For eachmodel, the training speed of each epoch is veryclose to its inference speed (during training, therewould be extra time cost for operations like theback propagation), thus we can easily know thatour model needs less time for training since ourmodel has a far less epoch number.5 ConclusionsIn this study, we propose a novel table ﬁlling basedRTE model that extracts triples based on two kindsof global features. The main contributions of ourwork are listed as follows. First, we make use ofthe global associations of relations and of tokenpairs. Experiments show these two kinds of globalfeatures are much helpful for performance. Second,our model works well on extracting triples fromcomplex sentences containing overlapping triplesor multiple triples. Third, our model is evaluated onthree benchmark datasets. Extensive experimentsshow that it consistently outperforms all the com-pared strong baselines and achieves state-of-the-artresults. Besides, our model has a competitive infer
 table ﬁlling basedRTE model that extracts triples based on two kindsof global features. The main contributions of ourwork are listed as follows. First, we make use ofthe global associations of relations and of tokenpairs. Experiments show these two kinds of globalfeatures are much helpful for performance. Second,our model works well on extracting triples fromcomplex sentences containing overlapping triplesor multiple triples. Third, our model is evaluated onthree benchmark datasets. Extensive experimentsshow that it consistently outperforms all the com-pared strong baselines and achieves state-of-the-artresults. Besides, our model has a competitive infer-ence speed and a moderate parameter size.AcknowledgmentsThis work is supported by the National NaturalScience Foundation of China (No.61572120 andNo.U1708261), the Fundamental Research Fundsfor the Central Universities (No.N181602013 andNo.N2016006), Shenyang Medical Imaging Pro-cessing Engineering Technology Research Cen-ter (17-134-8-00), Ten Thousand Talent Program(No.ZX20200035), and Liaoning Distinguished
-ence speed and a moderate parameter size.AcknowledgmentsThis work is supported by the National NaturalScience Foundation of China (No.61572120 andNo.U1708261), the Fundamental Research Fundsfor the Central Universities (No.N181602013 andNo.N2016006), Shenyang Medical Imaging Pro-cessing Engineering Technology Research Cen-ter (17-134-8-00), Ten Thousand Talent Program(No.ZX20200035), and Liaoning DistinguishedProfessor (No.XLYC1902057). ReferencesGiannis Bekoulis, Johannes Deleu, Thomas Demeester,and Chris Develder. 2018. Joint entity recogni-tion and relation extraction as a multi-head selec-tion problem. Expert Systems With Applications ,114:34–45.Yee Seng Chan and Dan Roth. 2011. Exploitingsyntactico-semantic structures for relation extrac-tion. In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies
Professor (No.XLYC1902057). ReferencesGiannis Bekoulis, Johannes Deleu, Thomas Demeester,and Chris Develder. 2018. Joint entity recogni-tion and relation extraction as a multi-head selec-tion problem. Expert Systems With Applications ,114:34–45.Yee Seng Chan and Dan Roth. 2011. Exploitingsyntactico-semantic structures for relation extrac-tion. In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies , pages 551–560.Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina N. Toutanova. 2018. Bert: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers) , pages4171–4186.Markus Eberts and Adrian Ulges. 2019. Span-basedjoint entity and relation extraction with transformerpre
 , pages 551–560.Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina N. Toutanova. 2018. Bert: Pre-training ofdeep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers) , pages4171–4186.Markus Eberts and Adrian Ulges. 2019. Span-basedjoint entity and relation extraction with transformerpre-training. In ECAI , pages 2006–2013.Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma. 2019.Graphrel: Modeling text as relational graphs forjoint entity and relation extraction. In Proceedingsof the 57th Annual Meeting of the Association forComputational Linguistics , pages 1409–1418.Claire Gardent, Anastasia Shimorina, Shashi Narayan,and Laura Perez-Beltrachini. 2017. Creating train-ing corpora for nlg micro
-training. In ECAI , pages 2006–2013.Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma. 2019.Graphrel: Modeling text as relational graphs forjoint entity and relation extraction. In Proceedingsof the 57th Annual Meeting of the Association forComputational Linguistics , pages 1409–1418.Claire Gardent, Anastasia Shimorina, Shashi Narayan,and Laura Perez-Beltrachini. 2017. Creating train-ing corpora for nlg micro-planners. In Proceedingsof the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers) ,pages 179–188.Pankaj Gupta, Hinrich Schütze, and Bernt Andrassy.2016. Table ﬁlling multi-task recurrent neural net-work for joint entity and relation extraction. In Pro-ceedings of COLING 2016, the 26th InternationalConference on Computational Linguistics: Techni-cal Papers , pages 2537–2547, Osaka, Japan.
-planners. In Proceedingsof the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers) ,pages 179–188.Pankaj Gupta, Hinrich Schütze, and Bernt Andrassy.2016. Table ﬁlling multi-task recurrent neural net-work for joint entity and relation extraction. In Pro-ceedings of COLING 2016, the 26th InternationalConference on Computational Linguistics: Techni-cal Papers , pages 2537–2547, Osaka, Japan. TheCOLING 2016 Organizing Committee.Diederik P. Kingma and Jimmy Ba. 2015. Adam: Amethod for stochastic optimization. In 3rd Inter-national Conference on Learning Representations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015,Conference Track Proceedings .Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, AriannaYuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019.Entity-relation extraction as multi-turn question
 TheCOLING 2016 Organizing Committee.Diederik P. Kingma and Jimmy Ba. 2015. Adam: Amethod for stochastic optimization. In 3rd Inter-national Conference on Learning Representations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015,Conference Track Proceedings .Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, AriannaYuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019.Entity-relation extraction as multi-turn question an-swering. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics ,pages 1340–1350.Makoto Miwa and Mohit Bansal. 2016. End-to-end re-lation extraction using LSTMs on sequences and treestructures. In Proceedings of the 54th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers) , pages 1105–1116, Berlin,Germany. Association for Computational Linguis-tics.Tapas Nayak and Hwee Tou
 an-swering. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics ,pages 1340–1350.Makoto Miwa and Mohit Bansal. 2016. End-to-end re-lation extraction using LSTMs on sequences and treestructures. In Proceedings of the 54th Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers) , pages 1105–1116, Berlin,Germany. Association for Computational Linguis-tics.Tapas Nayak and Hwee Tou Ng. 2020. Effective mod-eling of encoder-decoder architecture for joint en-tity and relation extraction. In The Thirty-FourthAAAI Conference on Artiﬁcial Intelligence, AAAI2020, The Thirty-Second Innovative Applications ofArtiﬁcial Intelligence Conference, IAAI 2020, TheTenth AAAI Symposium on Educational Advancesin Artiﬁcial Intelligence, EAAI 2020, New York, NY,USA, February 7-12, 2020 , pages 8528–8535
 Ng. 2020. Effective mod-eling of encoder-decoder architecture for joint en-tity and relation extraction. In The Thirty-FourthAAAI Conference on Artiﬁcial Intelligence, AAAI2020, The Thirty-Second Innovative Applications ofArtiﬁcial Intelligence Conference, IAAI 2020, TheTenth AAAI Symposium on Educational Advancesin Artiﬁcial Intelligence, EAAI 2020, New York, NY,USA, February 7-12, 2020 , pages 8528–8535. AAAIPress.Jeffrey Pennington, Richard Socher, and ChristopherManning. 2014. Glove: Global vectors for word rep-resentation. In Proceedings of the 2014 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP) , pages 1532–1543.Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, Xian-grong Zeng, and Shengping Liu. 2020. Joint entityand relation extraction with set prediction networks.CoRR , abs/2011.0
. AAAIPress.Jeffrey Pennington, Richard Socher, and ChristopherManning. 2014. Glove: Global vectors for word rep-resentation. In Proceedings of the 2014 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP) , pages 1532–1543.Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, Xian-grong Zeng, and Shengping Liu. 2020. Joint entityand relation extraction with set prediction networks.CoRR , abs/2011.01675.Kai Sun, Richong Zhang, Samuel Mensah, YongyiMao, and Xudong Liu. 2020. Recurrent interactionnetwork for jointly extracting entities and classify-ing relations. In Proceedings of the 2020 Confer-ence on Empirical Methods in Natural LanguageProcessing, EMNLP 2020, Online, November 16-20,2020 , pages 3722–3732. Association for Computa-tional Linguistics.Kai Sun, Richong Zhang, Samuel Mensah, YongyiMao
1675.Kai Sun, Richong Zhang, Samuel Mensah, YongyiMao, and Xudong Liu. 2020. Recurrent interactionnetwork for jointly extracting entities and classify-ing relations. In Proceedings of the 2020 Confer-ence on Empirical Methods in Natural LanguageProcessing, EMNLP 2020, Online, November 16-20,2020 , pages 3722–3732. Association for Computa-tional Linguistics.Kai Sun, Richong Zhang, Samuel Mensah, YongyiMao, and Xudong Liu. 2021. Progressive multitasklearning with controlled information ﬂow for jointentity and relation extraction. In Association for theAdvancement of Artiﬁcial Intelligence (AAAI) .Ryuichi Takanobu, Tianyang Zhang, Jiexi Liu, andMinlie Huang. 2019. A hierarchical framework forrelation extraction with reinforcement learning. Pro-ceedings of the AAAI Conference on Artiﬁcial Intel-ligence , 33(1):7072–7079.Ashish Vaswani, No
, and Xudong Liu. 2021. Progressive multitasklearning with controlled information ﬂow for jointentity and relation extraction. In Association for theAdvancement of Artiﬁcial Intelligence (AAAI) .Ryuichi Takanobu, Tianyang Zhang, Jiexi Liu, andMinlie Huang. 2019. A hierarchical framework forrelation extraction with reinforcement learning. Pro-ceedings of the AAAI Conference on Artiﬁcial Intel-ligence , 33(1):7072–7079.Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 5998–6008.Yucheng Wang, Bowen Yu, Yueyang Zhang, TingwenLiu, Hongs
am Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Pro-cessing Systems 30: Annual Conference on NeuralInformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 5998–6008.Yucheng Wang, Bowen Yu, Yueyang Zhang, TingwenLiu, Hongsong Zhu, and Limin Sun. 2020.TPLinker: Single-stage joint extraction of entitiesand relations through token pair linking. In Proceed-ings of the 28th International Conference on Com-putational Linguistics , pages 1572–1582, Barcelona,Spain (Online).Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, andYi Chang. 2020. A novel cascade binary tagging framework for relational triple extraction. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics
ong Zhu, and Limin Sun. 2020.TPLinker: Single-stage joint extraction of entitiesand relations through token pair linking. In Proceed-ings of the 28th International Conference on Com-putational Linguistics , pages 1572–1582, Barcelona,Spain (Online).Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, andYi Chang. 2020. A novel cascade binary tagging framework for relational triple extraction. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics , pages 1476–1488, Online. Association for Computational Lin-guistics.Bowen Yu, Zhenyu Zhang, Xiaobo Shu, Tingwen Liu,Yubin Wang, Bin Wang, and Sujian Li. 2019. Jointextraction of entities and relations based on a noveldecomposition strategy. In ECAI , pages 2282–2289.Yue Yuan, Xiaofei Zhou, Shirui Pan, Qiannan Zhu,Zeliang Song, and Li Guo. 2020. A relation-speci
 , pages 1476–1488, Online. Association for Computational Lin-guistics.Bowen Yu, Zhenyu Zhang, Xiaobo Shu, Tingwen Liu,Yubin Wang, Bin Wang, and Sujian Li. 2019. Jointextraction of entities and relations based on a noveldecomposition strategy. In ECAI , pages 2282–2289.Yue Yuan, Xiaofei Zhou, Shirui Pan, Qiannan Zhu,Zeliang Song, and Li Guo. 2020. A relation-speciﬁcattention network for joint entity and relation ex-traction. In Proceedings of the Twenty-Ninth Inter-national Joint Conference on Artiﬁcial Intelligence ,volume 4, pages 4054–4060.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella. 2003. Kernel methods for relation ex-traction. Journal of Machine Learning Research ,3(6):1083–1106.Daojian Zeng, Haoran Zhang, and Qianying Liu. 2020.Copymtl: Copy mechanism for joint extraction ofentities and
ﬁcattention network for joint entity and relation ex-traction. In Proceedings of the Twenty-Ninth Inter-national Joint Conference on Artiﬁcial Intelligence ,volume 4, pages 4054–4060.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella. 2003. Kernel methods for relation ex-traction. Journal of Machine Learning Research ,3(6):1083–1106.Daojian Zeng, Haoran Zhang, and Qianying Liu. 2020.Copymtl: Copy mechanism for joint extraction ofentities and relations with multi-task learning. Pro-ceedings of the AAAI Conference on Artiﬁcial Intel-ligence , 34(5):9507–9514.Xiangrong Zeng, Shizhu He, Daojian Zeng, Kang Liu,Shengping Liu, and Jun Zhao. 2019. Learning theextraction order of multiple relational facts in a sen-tence with reinforcement learning. In Proceedingsof the 2019 Conference on Empirical Methods inNatural Language Processing and the 9th Interna-tional Joint Conference on Natural Language Pro-
 relations with multi-task learning. Pro-ceedings of the AAAI Conference on Artiﬁcial Intel-ligence , 34(5):9507–9514.Xiangrong Zeng, Shizhu He, Daojian Zeng, Kang Liu,Shengping Liu, and Jun Zhao. 2019. Learning theextraction order of multiple relational facts in a sen-tence with reinforcement learning. In Proceedingsof the 2019 Conference on Empirical Methods inNatural Language Processing and the 9th Interna-tional Joint Conference on Natural Language Pro-cessing (EMNLP-IJCNLP) , pages 367–377.Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu,and Jun Zhao. 2018. Extracting relational facts byan end-to-end neural model with copy mechanism.InProceedings of the 56th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers) , volume 1, pages 506–514.Meishan Zhang, Yue Zhang, and Guohong Fu. 2017.End-to-end neural relation extraction with
cessing (EMNLP-IJCNLP) , pages 367–377.Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu,and Jun Zhao. 2018. Extracting relational facts byan end-to-end neural model with copy mechanism.InProceedings of the 56th Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers) , volume 1, pages 506–514.Meishan Zhang, Yue Zhang, and Guohong Fu. 2017.End-to-end neural relation extraction with global op-timization. In Proceedings of the 2017 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1730–1740, Copenhagen, Denmark. As-sociation for Computational Linguistics.Suncong Zheng, Feng Wang, Hongyun Bao, YuexingHao, Peng Zhou, and Bo Xu. 2017. Joint extrac-tion of entities and relations based on a novel tag-ging scheme. In Proceedings of the 55th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers) ,
 global op-timization. In Proceedings of the 2017 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1730–1740, Copenhagen, Denmark. As-sociation for Computational Linguistics.Suncong Zheng, Feng Wang, Hongyun Bao, YuexingHao, Peng Zhou, and Bo Xu. 2017. Joint extrac-tion of entities and relations based on a novel tag-ging scheme. In Proceedings of the 55th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers) , volume 1, pages1227–1236.GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005. Exploring various knowledge in relation ex-traction. In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL’05) , pages 427–434.
 volume 1, pages1227–1236.GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005. Exploring various knowledge in relation ex-traction. In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL’05) , pages 427–434.
