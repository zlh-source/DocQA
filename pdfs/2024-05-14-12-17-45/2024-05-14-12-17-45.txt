A Conditional Cascade Model for Relational Triple ExtractionFeiliang Ren∗renfeiliang@cse.neu.edu.cnNortheastern UniversityShenyang, ChinaLonghui Zhang∗Northeastern UniversityShenyang, ChinaShujuan YinNortheastern UniversityShenyang, ChinaXiaofeng ZhaoNortheastern UniversityShenyang, ChinaShilei LiuNortheastern UniversityShenyang, ChinaBochao LiNortheastern UniversityShenyang, ChinaABSTRACTTagging based methods are one of the mainstream methods in rela-tional triple extraction. However, most of them suffer from the classimbalance issue greatly. Here we propose a novel tagging basedmodel that addresses this issue from following two aspects. First, atthe model level, we propose a three-step extraction framework thatcan reduce the total number of samples greatly, which implicitlydecreases the severity of the mentioned issue. Second, at the intra-model level, we propose a confidence threshold based cross entropyloss that can directly neglect some samples in the major classes. Weevaluate the proposed model on NYT and WebNLG. Extensive exper-iments show that it can address the mentioned issue effectively andachieves state-of-the-art results on both datasets. The source code
 classimbalance issue greatly. Here we propose a novel tagging basedmodel that addresses this issue from following two aspects. First, atthe model level, we propose a three-step extraction framework thatcan reduce the total number of samples greatly, which implicitlydecreases the severity of the mentioned issue. Second, at the intra-model level, we propose a confidence threshold based cross entropyloss that can directly neglect some samples in the major classes. Weevaluate the proposed model on NYT and WebNLG. Extensive exper-iments show that it can address the mentioned issue effectively andachieves state-of-the-art results on both datasets. The source codeof our model is available at: https://github.com/neukg/ConCasRTE.CCS CONCEPTS•Computing methodologies →Information extraction .KEYWORDSrelational triple extraction, class imbalance issueACM Reference Format:Feiliang Ren, Longhui Zhang, Shujuan Yin, Xiaofeng Zhao, Shilei Liu,and Bochao Li. 2021. A Conditional Cascade Model for Relational TripleExtraction. In Proceedings of the 30th ACM International Conference on In-formation and Knowledge Management (CIKM ’21), November 1–5, 20
of our model is available at: https://github.com/neukg/ConCasRTE.CCS CONCEPTS•Computing methodologies →Information extraction .KEYWORDSrelational triple extraction, class imbalance issueACM Reference Format:Feiliang Ren, Longhui Zhang, Shujuan Yin, Xiaofeng Zhao, Shilei Liu,and Bochao Li. 2021. A Conditional Cascade Model for Relational TripleExtraction. In Proceedings of the 30th ACM International Conference on In-formation and Knowledge Management (CIKM ’21), November 1–5, 2021,Virtual Event, QLD, Australia. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3459637.34820451 INTRODUCTIONTaking unstructured text (often sentences) as input, relational tripleextraction (RTE for short) aims to extract triples that are in the formof (subject, relation, object ), where both subject andobject are entitiesand they are connected semantically by relation . RTE is importantfor some tasks like automatic knowledge graph construction.∗Both authors contribute equally to this research
21,Virtual Event, QLD, Australia. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3459637.34820451 INTRODUCTIONTaking unstructured text (often sentences) as input, relational tripleextraction (RTE for short) aims to extract triples that are in the formof (subject, relation, object ), where both subject andobject are entitiesand they are connected semantically by relation . RTE is importantfor some tasks like automatic knowledge graph construction.∗Both authors contribute equally to this research and are listed randomly.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.CIKM ’21, November 1–
 and are listed randomly.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.CIKM ’21, November 1–5, 2021, Virtual Event, QLD, Australia©2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-8446-9/21/11. . . $15.00https://doi.org/10.1145/3459637.3482045Nowadays, the methods that jointly extract entities and relationsare dominant in RTE. Lots of novel joint extraction methods havebeen proposed [ 1,5,
5, 2021, Virtual Event, QLD, Australia©2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-8446-9/21/11. . . $15.00https://doi.org/10.1145/3459637.3482045Nowadays, the methods that jointly extract entities and relationsare dominant in RTE. Lots of novel joint extraction methods havebeen proposed [ 1,5,6,14,21–23,25,29], and they achieve muchbetter results than the pipeline based methods. According to the ex-traction routes taken, most of existing joint extraction methods canbe roughly classified into following three kinds. (i) Tagging basedmethods [21,22,29] that often use binary (positive and negative)tag sequences to determine: (1) the start and end tokens of entities,and (2) all the relations for each entity pair. (ii) Table-filling basedmethods [8,13,20,28] that maintain
6,14,21–23,25,29], and they achieve muchbetter results than the pipeline based methods. According to the ex-traction routes taken, most of existing joint extraction methods canbe roughly classified into following three kinds. (i) Tagging basedmethods [21,22,29] that often use binary (positive and negative)tag sequences to determine: (1) the start and end tokens of entities,and (2) all the relations for each entity pair. (ii) Table-filling basedmethods [8,13,20,28] that maintain a table for each relation andthe items in a table usually denotes the start and end positions oftwo entities (or even the types of these entities) that possess thisrelation. (iii) Seq2Seq based methods [15,25–27] that view a tripleas a token sequence and generate a triple in some orders, such asfirst generate a relation, then generate entities, etc.Recently, tagging based methods are attracting more and moreresearch interests due to their superiority in both the performanceand the ability of extracting triples from complex sentences thatcontain overlapping triples [ 27] or multiple triples. However,
 a table for each relation andthe items in a table usually denotes the start and end positions oftwo entities (or even the types of these entities) that possess thisrelation. (iii) Seq2Seq based methods [15,25–27] that view a tripleas a token sequence and generate a triple in some orders, such asfirst generate a relation, then generate entities, etc.Recently, tagging based methods are attracting more and moreresearch interests due to their superiority in both the performanceand the ability of extracting triples from complex sentences thatcontain overlapping triples [ 27] or multiple triples. However, inthese methods, the negative class usually contains far more samplesthan the positive class since there are always much more non-entitytokens in a sentence and most entity pairs possess only a very smallnumber of relations. Therefore, these methods suffer from the classimbalance issue greatly: the major classes (here is the negative class)have far more samples than the minor classes (here is the positiveclass). This issue is very harmful to performance because it makesthe training inefficient and the trained model biased towards themajor classes [ 3,9]. Most recent methods for addressing this issuecan be divided into following two kinds [ 3]: (
 inthese methods, the negative class usually contains far more samplesthan the positive class since there are always much more non-entitytokens in a sentence and most entity pairs possess only a very smallnumber of relations. Therefore, these methods suffer from the classimbalance issue greatly: the major classes (here is the negative class)have far more samples than the minor classes (here is the positiveclass). This issue is very harmful to performance because it makesthe training inefficient and the trained model biased towards themajor classes [ 3,9]. Most recent methods for addressing this issuecan be divided into following two kinds [ 3]: (i) re-sampling basedmethods[ 31] that adjust the number of samples directly by addingrepetitive data for the minor classes or removing data for the majorclasses; and (ii) cost-sensitive re-weighting based methods [ 3,11,12]that influence the loss function by assigning relatively higher coststo samples from minor classes. However, as [ 3] point out that thefirst ones are error-prone, and the second ones often make someassumptions on the sample difficulty and data distribution, butthese assumptions do not always hold.Obviously, the key of addressing the class imbalance issue
i) re-sampling basedmethods[ 31] that adjust the number of samples directly by addingrepetitive data for the minor classes or removing data for the majorclasses; and (ii) cost-sensitive re-weighting based methods [ 3,11,12]that influence the loss function by assigning relatively higher coststo samples from minor classes. However, as [ 3] point out that thefirst ones are error-prone, and the second ones often make someassumptions on the sample difficulty and data distribution, butthese assumptions do not always hold.Obviously, the key of addressing the class imbalance issue is tonarrow the number gap between samples in the classes of major andminor. Following this line, we propose ConCasRTE , aConditionalCascade RTEmodel that can address this issue existed in the taggingbased RTE methods from following two aspects. First, we proposea three-step extraction framework. Compared with existing two-step extraction framework [ 21,22] that first extracts subjects thenextracts objects and relations simultaneously based on the subjects 0 1 0 0 0 0 1 00 0 1 0 0 0 1 0Object-T agger0 0
 is tonarrow the number gap between samples in the classes of major andminor. Following this line, we propose ConCasRTE , aConditionalCascade RTEmodel that can address this issue existed in the taggingbased RTE methods from following two aspects. First, we proposea three-step extraction framework. Compared with existing two-step extraction framework [ 21,22] that first extracts subjects thenextracts objects and relations simultaneously based on the subjects 0 1 0 0 0 0 1 00 0 1 0 0 0 1 0Object-T agger0 0 0 0 0 0 1 00 0 0 0 0 0 1 0RE1location0owner af filiationSubject-T agger0T irstrupDenmrakArhusAirportislocated,inEncoderStartEndRelationshihihihivkvjsubobjrelsoFigure 1: Model Architectureextracted, this new framework generates far less samples. Thusit narrows the mentioned number gap implicitly due to the factthat the less samples there are, the less possibility there would bea large mentioned number gap. Second, we propose a confidencethreshold based cross
 0 0 0 0 1 00 0 0 0 0 0 1 0RE1location0owner af filiationSubject-T agger0T irstrupDenmrakArhusAirportislocated,inEncoderStartEndRelationshihihihivkvjsubobjrelsoFigure 1: Model Architectureextracted, this new framework generates far less samples. Thusit narrows the mentioned number gap implicitly due to the factthat the less samples there are, the less possibility there would bea large mentioned number gap. Second, we propose a confidencethreshold based cross entropy loss function that can directly neglectlots of samples in the major classes, which narrows the mentionednumber gap explicitly. We evaluate ConCasRTE on two benchmarkdatasets, namely NYT and WebNLG. Extensive experiments show itis effective and achieves the state-of-the-art results on both datasets.2 METHODOLOGYThe architecture of ConCasRTE is shown in Figure 1. There are fourmain modules in it: an Encoder module, a Subject-Tagger module,anObject-Tagger module, and a Relation Extraction module ( REforshort). These modules work in a cascade manner. And the latterthree modules
 entropy loss function that can directly neglectlots of samples in the major classes, which narrows the mentionednumber gap explicitly. We evaluate ConCasRTE on two benchmarkdatasets, namely NYT and WebNLG. Extensive experiments show itis effective and achieves the state-of-the-art results on both datasets.2 METHODOLOGYThe architecture of ConCasRTE is shown in Figure 1. There are fourmain modules in it: an Encoder module, a Subject-Tagger module,anObject-Tagger module, and a Relation Extraction module ( REforshort). These modules work in a cascade manner. And the latterthree modules form a three-step extraction framework: first extractssubjects, then extracts objects, and finally extracts relations.Encoder Firstly, a pre-trained BERT-Base (Cased) model [ 4] is usedto generate an initial representation (denoted as h𝑖∈ R𝑑ℎ) foreach token in an input sentence. Then the context features forsubjects, objects, and relations are generated with Eq. (1), whereW(.)∈R𝑑ℎ×𝑑ℎare trainable weights, and b(.)∈R𝑑ℎare biases.h𝑖𝑠𝑢𝑏=W𝑠𝑢𝑏h𝑖+b𝑠𝑢𝑏h𝑖𝑜𝑏
 form a three-step extraction framework: first extractssubjects, then extracts objects, and finally extracts relations.Encoder Firstly, a pre-trained BERT-Base (Cased) model [ 4] is usedto generate an initial representation (denoted as h𝑖∈ R𝑑ℎ) foreach token in an input sentence. Then the context features forsubjects, objects, and relations are generated with Eq. (1), whereW(.)∈R𝑑ℎ×𝑑ℎare trainable weights, and b(.)∈R𝑑ℎare biases.h𝑖𝑠𝑢𝑏=W𝑠𝑢𝑏h𝑖+b𝑠𝑢𝑏h𝑖𝑜𝑏𝑗=W𝑜𝑏𝑗h𝑖+b𝑜𝑏𝑗h𝑖𝑟𝑒𝑙=W𝑟𝑒𝑙h𝑖+b𝑟𝑒𝑙(1)Subject/Object Taggers Taking each token in a sentence as input,Subject-Tagger uses two binary tag sequences to determine whetherit is the start and end tokens of a subject, as shown in Eq.(2).𝑝𝑠,𝑖𝑠𝑡𝑎𝑟𝑡=𝜎(W𝑠𝑠𝑡𝑎𝑟𝑡 h𝑖𝑠𝑢𝑏+b𝑠𝑠𝑡𝑎𝑟𝑡)𝑝𝑠,𝑖𝑒𝑛𝑑=𝜎(W𝑠𝑒𝑛𝑑h𝑖𝑠𝑢𝑏+b𝑠𝑒𝑛
𝑗=W𝑜𝑏𝑗h𝑖+b𝑜𝑏𝑗h𝑖𝑟𝑒𝑙=W𝑟𝑒𝑙h𝑖+b𝑟𝑒𝑙(1)Subject/Object Taggers Taking each token in a sentence as input,Subject-Tagger uses two binary tag sequences to determine whetherit is the start and end tokens of a subject, as shown in Eq.(2).𝑝𝑠,𝑖𝑠𝑡𝑎𝑟𝑡=𝜎(W𝑠𝑠𝑡𝑎𝑟𝑡 h𝑖𝑠𝑢𝑏+b𝑠𝑠𝑡𝑎𝑟𝑡)𝑝𝑠,𝑖𝑒𝑛𝑑=𝜎(W𝑠𝑒𝑛𝑑h𝑖𝑠𝑢𝑏+b𝑠𝑒𝑛𝑑)(2)where𝑝𝑠,𝑖𝑠𝑡𝑎𝑟𝑡and𝑝𝑠,𝑖𝑒𝑛𝑑denote the probabilities of the 𝑖-th inputtoken being the start and end tokens of a subject respectively.Subsequently, taking each extracted subject as an input priorcondition, Object-Tagger extracts all objects of this subject. It alsouses two binary tag sequences to determine whether a token in theinput sentence is the start and end tokens of an object that can forma (subject, object ) pair with the input subject, as shown in Eq.(3).𝑝𝑜,𝑖,𝑘𝑠𝑡𝑎𝑟𝑡=𝜎
𝑑)(2)where𝑝𝑠,𝑖𝑠𝑡𝑎𝑟𝑡and𝑝𝑠,𝑖𝑒𝑛𝑑denote the probabilities of the 𝑖-th inputtoken being the start and end tokens of a subject respectively.Subsequently, taking each extracted subject as an input priorcondition, Object-Tagger extracts all objects of this subject. It alsouses two binary tag sequences to determine whether a token in theinput sentence is the start and end tokens of an object that can forma (subject, object ) pair with the input subject, as shown in Eq.(3).𝑝𝑜,𝑖,𝑘𝑠𝑡𝑎𝑟𝑡=𝜎(W𝑜𝑠𝑡𝑎𝑟𝑡(h𝑖𝑜𝑏𝑗◦v𝑘𝑠)+b𝑜𝑠𝑡𝑎𝑟𝑡)𝑝𝑜,𝑖,𝑘𝑒𝑛𝑑=𝜎(W𝑜𝑒𝑛𝑑(h𝑖𝑜𝑏𝑗◦v𝑘𝑠)+b𝑜𝑒𝑛𝑑)(3)where v𝑘𝑠is the vector representation of the 𝑘-th input subject and isobtained by simply averaging all its tokens’ vector representations;𝑝𝑜,𝑖,𝑘𝑠𝑡𝑎𝑟𝑡and𝑝𝑜,𝑖,𝑘𝑒𝑛𝑑denote the probabilities of the 𝑖-th input tokenbeing the start and
(W𝑜𝑠𝑡𝑎𝑟𝑡(h𝑖𝑜𝑏𝑗◦v𝑘𝑠)+b𝑜𝑠𝑡𝑎𝑟𝑡)𝑝𝑜,𝑖,𝑘𝑒𝑛𝑑=𝜎(W𝑜𝑒𝑛𝑑(h𝑖𝑜𝑏𝑗◦v𝑘𝑠)+b𝑜𝑒𝑛𝑑)(3)where v𝑘𝑠is the vector representation of the 𝑘-th input subject and isobtained by simply averaging all its tokens’ vector representations;𝑝𝑜,𝑖,𝑘𝑠𝑡𝑎𝑟𝑡and𝑝𝑜,𝑖,𝑘𝑒𝑛𝑑denote the probabilities of the 𝑖-th input tokenbeing the start and end tokens of an object that can form an entitypair with the 𝑘-th input subject;◦denotes the hadamard productoperation.RETaking each ( subject, object ) pair as input, REextracts all rela-tions for this input entity pair, as shown in Eq. (4).p𝑘,𝑗𝑟=1|𝐿𝑂𝐶|∑︁𝑖∈LOC𝜎W𝑟h𝑖𝑟𝑒𝑙◦v𝑘𝑠◦v𝑗𝑜+b𝑘,𝑗𝑟LOC=[𝑙𝑜𝑐𝑘,𝑠𝑡𝑎𝑟𝑡𝑠,𝑙𝑜𝑐𝑘,𝑒
 end tokens of an object that can form an entitypair with the 𝑘-th input subject;◦denotes the hadamard productoperation.RETaking each ( subject, object ) pair as input, REextracts all rela-tions for this input entity pair, as shown in Eq. (4).p𝑘,𝑗𝑟=1|𝐿𝑂𝐶|∑︁𝑖∈LOC𝜎W𝑟h𝑖𝑟𝑒𝑙◦v𝑘𝑠◦v𝑗𝑜+b𝑘,𝑗𝑟LOC=[𝑙𝑜𝑐𝑘,𝑠𝑡𝑎𝑟𝑡𝑠,𝑙𝑜𝑐𝑘,𝑒𝑛𝑑𝑠]∪[𝑙𝑜𝑐𝑗,𝑠𝑡𝑎𝑟𝑡𝑜,𝑙𝑜𝑐𝑗,𝑒𝑛𝑑𝑜](4)where v𝑘𝑠andv𝑗𝑜are vector representations of the 𝑘-th subject and𝑗-th object, and v𝑗𝑜is obtained by the same way as v𝑘𝑠;p𝑘,𝑗𝑟∈R|R|isa probability sequence, |R|is the size of relation set R, and each iteminp𝑘,𝑗𝑟corresponds to a specific relation and is used to determinewhether this relation should be assigned to the input entity pair;𝑙𝑜𝑐
𝑛𝑑𝑠]∪[𝑙𝑜𝑐𝑗,𝑠𝑡𝑎𝑟𝑡𝑜,𝑙𝑜𝑐𝑗,𝑒𝑛𝑑𝑜](4)where v𝑘𝑠andv𝑗𝑜are vector representations of the 𝑘-th subject and𝑗-th object, and v𝑗𝑜is obtained by the same way as v𝑘𝑠;p𝑘,𝑗𝑟∈R|R|isa probability sequence, |R|is the size of relation set R, and each iteminp𝑘,𝑗𝑟corresponds to a specific relation and is used to determinewhether this relation should be assigned to the input entity pair;𝑙𝑜𝑐𝑘,𝑠𝑡𝑎𝑟𝑡𝑠 ,𝑙𝑜𝑐𝑘,𝑒𝑛𝑑𝑠 ,𝑙𝑜𝑐𝑗,𝑠𝑡𝑎𝑟𝑡𝑜 and𝑙𝑜𝑐𝑗,𝑒𝑛𝑑𝑜 denote the start and endpositions of the two input entities; LOC is the position range of theinput entity pair, and |𝐿𝑂𝐶|is the number of tokens in this pair.In Eq. (2)-(4),W𝑠(.),W𝑜(.)∈R1×𝑑ℎ,W𝑟∈R|R|×𝑑ℎare weights,b𝑠(.),b𝑜(.)∈R1,b(,)𝑟∈R|R|are
𝑘,𝑠𝑡𝑎𝑟𝑡𝑠 ,𝑙𝑜𝑐𝑘,𝑒𝑛𝑑𝑠 ,𝑙𝑜𝑐𝑗,𝑠𝑡𝑎𝑟𝑡𝑜 and𝑙𝑜𝑐𝑗,𝑒𝑛𝑑𝑜 denote the start and endpositions of the two input entities; LOC is the position range of theinput entity pair, and |𝐿𝑂𝐶|is the number of tokens in this pair.In Eq. (2)-(4),W𝑠(.),W𝑜(.)∈R1×𝑑ℎ,W𝑟∈R|R|×𝑑ℎare weights,b𝑠(.),b𝑜(.)∈R1,b(,)𝑟∈R|R|are biases, and 𝜎is a sigmoid function.Confidence Threshold based Loss Traditional loss functions likecross entropy usually assign lower costs to samples whose predic-tions are correct and the model is confident for these predictions(here we say a model is confident for a prediction if it assigns a veryhigh or a very low probability for this prediction). The major classesusually account for the majority of these low cost samples due tothe overwhelming number of samples in them. So it would bringfollowing two benefits if we directly neglect these low cost samples.First, most of the neglected samples would be in the major classes.Second, neglecting these samples wouldn’t have
 biases, and 𝜎is a sigmoid function.Confidence Threshold based Loss Traditional loss functions likecross entropy usually assign lower costs to samples whose predic-tions are correct and the model is confident for these predictions(here we say a model is confident for a prediction if it assigns a veryhigh or a very low probability for this prediction). The major classesusually account for the majority of these low cost samples due tothe overwhelming number of samples in them. So it would bringfollowing two benefits if we directly neglect these low cost samples.First, most of the neglected samples would be in the major classes.Second, neglecting these samples wouldn’t have much impact onthe model training since the predictions of these samples are cor-rect and confident . Accordingly, the class imbalance issue wouldbe alleviated greatly by such a neglect operation. Inspired by this,we propose a confidence threshold based cross entropy loss whichmakes a model only be trained by the samples whose predictions are not confident or incorrect, as shown in Eq. (5)- (7).𝑐𝑒′(𝑝,𝑡)=𝜉∗𝑐𝑒(𝑝,𝑡) (5)𝑐𝑒(𝑝,𝑡)=−[𝑡𝑙𝑜𝑔𝑝+(1−𝑡)𝑙𝑜𝑔(1−𝑝)] (6
 much impact onthe model training since the predictions of these samples are cor-rect and confident . Accordingly, the class imbalance issue wouldbe alleviated greatly by such a neglect operation. Inspired by this,we propose a confidence threshold based cross entropy loss whichmakes a model only be trained by the samples whose predictions are not confident or incorrect, as shown in Eq. (5)- (7).𝑐𝑒′(𝑝,𝑡)=𝜉∗𝑐𝑒(𝑝,𝑡) (5)𝑐𝑒(𝑝,𝑡)=−[𝑡𝑙𝑜𝑔𝑝+(1−𝑡)𝑙𝑜𝑔(1−𝑝)] (6)𝜉=(0,(𝑡−𝑇)(𝑝−𝑇)>0&|𝑝−0.5|>𝐶1,𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒(7)where𝑐𝑒′is the proposed loss; 𝑐𝑒is a basic binary cross entropy loss;𝑝∈(0,1)is a prediction probability and 𝑡∈{0,1}is its true tag; 𝜉isa switch coefficient to determine whether the model be trained byan input sample; 𝑇is a hyperparameter used to determine whethera prediction is assigned 1or0;𝐶∈[0
)𝜉=(0,(𝑡−𝑇)(𝑝−𝑇)>0&|𝑝−0.5|>𝐶1,𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒(7)where𝑐𝑒′is the proposed loss; 𝑐𝑒is a basic binary cross entropy loss;𝑝∈(0,1)is a prediction probability and 𝑡∈{0,1}is its true tag; 𝜉isa switch coefficient to determine whether the model be trained byan input sample; 𝑇is a hyperparameter used to determine whethera prediction is assigned 1or0;𝐶∈[0,0.5]is a hyperparameter andwe call it as confidence threshold ;|𝑝−0.5|>𝐶means the model isconfident for the prediction: the larger the confidence threshold isset, the higher confident degree of the model for its predictions isrequired; and(𝑡−𝑇)(𝑝−𝑇)>0means the prediction is correct.Finally, the proposed loss is used for training the modules ofSubject-Tagger ,Object-Tagger , and RE. The overall loss of ConCasRTEis defined as the sum of these separated losses. During training, wetake the popular teacher forcing strategy where the
,0.5]is a hyperparameter andwe call it as confidence threshold ;|𝑝−0.5|>𝐶means the model isconfident for the prediction: the larger the confidence threshold isset, the higher confident degree of the model for its predictions isrequired; and(𝑡−𝑇)(𝑝−𝑇)>0means the prediction is correct.Finally, the proposed loss is used for training the modules ofSubject-Tagger ,Object-Tagger , and RE. The overall loss of ConCasRTEis defined as the sum of these separated losses. During training, wetake the popular teacher forcing strategy where the ground truthsamples are used as input. To alleviate the exposure bias issue [ 20]caused by this strategy, we add some randomly generated noisesamples into the ground truth samples and use them together.3 EXPERIMENTS3.1 Experiment SettingsDatasets Here following two benchmark datasets are used: NYT [ 17]and WebNLG [ 7]. Both of them have two different versions accord-ing to following two annotation standards: 1) annotating the lasttoken of each entity, and 2) annotating the whole entity span. Fol-lowing TPLinker [20], we denote the datasets based
 ground truthsamples are used as input. To alleviate the exposure bias issue [ 20]caused by this strategy, we add some randomly generated noisesamples into the ground truth samples and use them together.3 EXPERIMENTS3.1 Experiment SettingsDatasets Here following two benchmark datasets are used: NYT [ 17]and WebNLG [ 7]. Both of them have two different versions accord-ing to following two annotation standards: 1) annotating the lasttoken of each entity, and 2) annotating the whole entity span. Fol-lowing TPLinker [20], we denote the datasets based on the firststandard as NYT∗and WebNLG∗, and the datasets based on thesecond standard as NYT and WebNLG. Some statistics of thesedatasets are shown in Table 1: EPO,SEO, and Normal refer to entitypair overlapping ,single entity overlapping , and no overlapped triplesrespectively [ 27]. Note a sentence can belong to both EPO andSEO.Evaluation Metrics The standard micro precision, recall, and F1score are used to evaluate the results. There are two match standardsfor the RTE task: (i) Partial Match : an extracted triplet is regarded ascorrect if the predicted
 on the firststandard as NYT∗and WebNLG∗, and the datasets based on thesecond standard as NYT and WebNLG. Some statistics of thesedatasets are shown in Table 1: EPO,SEO, and Normal refer to entitypair overlapping ,single entity overlapping , and no overlapped triplesrespectively [ 27]. Note a sentence can belong to both EPO andSEO.Evaluation Metrics The standard micro precision, recall, and F1score are used to evaluate the results. There are two match standardsfor the RTE task: (i) Partial Match : an extracted triplet is regarded ascorrect if the predicted relation and the head of both subject entityand object entity are correct; (ii) Exact Match : a triple is regarded ascorrect only when its entities and relation are completely matchedwith a correct triple. Here we follow [ 19–21]: use Partial Match onNYT∗and WebNLG∗, and use Exact Match on NYT and WebNLG.Implementation Details AdamW [ 10] is used to train ConCasRTE .All the hyperparameters are determined based on the results on thedevelopment set. Finally, they are set as follows. On NYT and NYT∗,the batch size is set to
 relation and the head of both subject entityand object entity are correct; (ii) Exact Match : a triple is regarded ascorrect only when its entities and relation are completely matchedwith a correct triple. Here we follow [ 19–21]: use Partial Match onNYT∗and WebNLG∗, and use Exact Match on NYT and WebNLG.Implementation Details AdamW [ 10] is used to train ConCasRTE .All the hyperparameters are determined based on the results on thedevelopment set. Finally, they are set as follows. On NYT and NYT∗,the batch size is set to 18 and epoch is set to 100. On WebNLG andWebNLG∗, the batch size is set to 6 and epoch is set to 50. On alldatasets, the learning rate is set to 1𝑒−5, the confidence threshold (𝐶in Eq. (7)) is set to 0.1, and all other thresholds are set to 0.5.Baselines Following strong state-of-the-art models are taken asbaselines: ETL-Span [22],WDec [14],RSAN [23],RIN[18
 18 and epoch is set to 100. On WebNLG andWebNLG∗, the batch size is set to 6 and epoch is set to 50. On alldatasets, the learning rate is set to 1𝑒−5, the confidence threshold (𝐶in Eq. (7)) is set to 0.1, and all other thresholds are set to 0.5.Baselines Following strong state-of-the-art models are taken asbaselines: ETL-Span [22],WDec [14],RSAN [23],RIN[18],PMEI [19],CasRel [21], and TPLinker [20]. We also implement a LSTM -encoderversion of ConCasRTE where 300-dimensional GloVe embeddings [ 16]and 2-layer stacked BiLSTM are used.Table 1: Statistics of datasets.CategoryNYT WebNLGTrain Test Train TestNormal 37013 3266 1596 246EPO 9782 978 227 26SEO 14735 129
],PMEI [19],CasRel [21], and TPLinker [20]. We also implement a LSTM -encoderversion of ConCasRTE where 300-dimensional GloVe embeddings [ 16]and 2-layer stacked BiLSTM are used.Table 1: Statistics of datasets.CategoryNYT WebNLGTrain Test Train TestNormal 37013 3266 1596 246EPO 9782 978 227 26SEO 14735 1297 3406 4573.2 Experimental ResultsMain Results The main experimental results are shown in Table 2.We can see that ConCasRTE is very effective. On all datasets andunder both match standards, it consistently outperforms all thecompared state-of-the-art baselines in term of F1. As for othermetrics, ConCasRTE achieves the best results on most of cases, andeven the exceptions are very close to the best results.Evaluations on Complex Sentences Here we evaluate ConCas-RTE’s ability for extracting triples from complex sentences thatcontain overlapping triples or multiple triples.
7 3406 4573.2 Experimental ResultsMain Results The main experimental results are shown in Table 2.We can see that ConCasRTE is very effective. On all datasets andunder both match standards, it consistently outperforms all thecompared state-of-the-art baselines in term of F1. As for othermetrics, ConCasRTE achieves the best results on most of cases, andeven the exceptions are very close to the best results.Evaluations on Complex Sentences Here we evaluate ConCas-RTE’s ability for extracting triples from complex sentences thatcontain overlapping triples or multiple triples. This ability is widelydiscussed by existing work, and can be viewed as a metric to evalu-ate the robustness of a model. To be fair, we conduct experimentson the same subsets as some previous best models [20, 21].The results are in Table 3, which demonstrate the great superi-ority of ConCasRTE for handling both kinds of complex sentences.On both datasets, it achieves much better results than the com-pared baselines. In fact, ConCasRTE inherits the main strengths ofexisting tagging based methods for extracting triples from complexsentences, but focuses more on well addresses the
 This ability is widelydiscussed by existing work, and can be viewed as a metric to evalu-ate the robustness of a model. To be fair, we conduct experimentson the same subsets as some previous best models [20, 21].The results are in Table 3, which demonstrate the great superi-ority of ConCasRTE for handling both kinds of complex sentences.On both datasets, it achieves much better results than the com-pared baselines. In fact, ConCasRTE inherits the main strengths ofexisting tagging based methods for extracting triples from complexsentences, but focuses more on well addresses the class imbalanceissue existed in these methods, thus it achieves much better results.Detailed Analyses Table 4 shows some detailed experimental re-sults about the proposed extraction framework and loss function.All these results are obtained when the BERT -based encoder used.First , we evaluate the effectiveness of the proposed extractionframework. To this end, we implement ConCasRTE 𝐶𝑒, a variant thatuses the basic binary cross entropy loss. Then we compare it withCasRel (the current best tagging based RTE model) since the maindifference between them is the extraction framework. We can seeConCasRTE 𝐶𝑒achieves
 class imbalanceissue existed in these methods, thus it achieves much better results.Detailed Analyses Table 4 shows some detailed experimental re-sults about the proposed extraction framework and loss function.All these results are obtained when the BERT -based encoder used.First , we evaluate the effectiveness of the proposed extractionframework. To this end, we implement ConCasRTE 𝐶𝑒, a variant thatuses the basic binary cross entropy loss. Then we compare it withCasRel (the current best tagging based RTE model) since the maindifference between them is the extraction framework. We can seeConCasRTE 𝐶𝑒achieves much better results on all datasets. In fact,the proposed framework can reduce the total number of samplesgreatly, which is much helpful for alleviating the class imbalanceissue . Taking a𝑙-token sentence as example, the number of samplesinConCasRTE is2𝑙+2𝑠𝑙+𝑛|𝑅|(𝑠is the number of subjects extracted,and𝑛is the number of all ( subject, object ) pairs). In this number,2𝑙,2𝑠𝑙, and𝑛|𝑅|are generated by the modules of Subject-Tagger ,Object-Tagger , and RErespectively. In CasRel , the number of
 much better results on all datasets. In fact,the proposed framework can reduce the total number of samplesgreatly, which is much helpful for alleviating the class imbalanceissue . Taking a𝑙-token sentence as example, the number of samplesinConCasRTE is2𝑙+2𝑠𝑙+𝑛|𝑅|(𝑠is the number of subjects extracted,and𝑛is the number of all ( subject, object ) pairs). In this number,2𝑙,2𝑠𝑙, and𝑛|𝑅|are generated by the modules of Subject-Tagger ,Object-Tagger , and RErespectively. In CasRel , the number of samplesis2𝑙+2𝑠𝑙|𝑅|, where 2𝑙and 2𝑠𝑙|𝑅|are generated by its modulesof subject extraction and object-relation extraction respectively.Usually𝑛≪𝑙, thus 2𝑙+2𝑠𝑙+𝑛|𝑅|≪ 2𝑙+2𝑠𝑙+𝑙|𝑅|<2𝑙+2𝑠𝑙|𝑅|. Andthere are 2𝑠+2𝑛+𝑡and 2𝑠+2𝑡samples in the positive classes ofConCasRTE andCasRel respectively ( 𝑡is the number of triples), andthe difference between these two
 samplesis2𝑙+2𝑠𝑙|𝑅|, where 2𝑙and 2𝑠𝑙|𝑅|are generated by its modulesof subject extraction and object-relation extraction respectively.Usually𝑛≪𝑙, thus 2𝑙+2𝑠𝑙+𝑛|𝑅|≪ 2𝑙+2𝑠𝑙+𝑙|𝑅|<2𝑙+2𝑠𝑙|𝑅|. Andthere are 2𝑠+2𝑛+𝑡and 2𝑠+2𝑡samples in the positive classes ofConCasRTE andCasRel respectively ( 𝑡is the number of triples), andthe difference between these two numbers can be negligible sinceboth are very small. So the number gap between samples in classesof positive and negative in ConCasRTE is much smaller than thatinCasRel , which makes the class imbalance issue alleviated greatly.Second , we evaluate the proposed loss function from the aspectsof ability for addressing the class imbalance issue and adaptability. Table 2: Main experiments.★means the results are produced by us by running the available source code.ModelPartial Match Exact MatchNYT∗WebNLG∗NYT WebNLGPrec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec
 numbers can be negligible sinceboth are very small. So the number gap between samples in classesof positive and negative in ConCasRTE is much smaller than thatinCasRel , which makes the class imbalance issue alleviated greatly.Second , we evaluate the proposed loss function from the aspectsof ability for addressing the class imbalance issue and adaptability. Table 2: Main experiments.★means the results are produced by us by running the available source code.ModelPartial Match Exact MatchNYT∗WebNLG∗NYT WebNLGPrec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1ETL-Span 84.9 72.3 78.1 84.0 91.5 87.6 85.5 71.7 78.0 84.3 82.0 83.1WDec – – – – – – 88.1 76.1 81.7 – – –RSAN – – – – – – 85.7 83.6 84.6 80.5 83.8 8
. F1ETL-Span 84.9 72.3 78.1 84.0 91.5 87.6 85.5 71.7 78.0 84.3 82.0 83.1WDec – – – – – – 88.1 76.1 81.7 – – –RSAN – – – – – – 85.7 83.6 84.6 80.5 83.8 82.1RIN 87.2 87.3 87.3 87.6 87.0 87.3 83.9 85.5 84.7 77.3 76.8 77.0CasRel𝐿𝑆𝑇𝑀 84.2 83.0 83.6 86.9 80.6 83.7 – – – – – –PMEI𝐿𝑆𝑇𝑀 88.7 86.8 87.
2.1RIN 87.2 87.3 87.3 87.6 87.0 87.3 83.9 85.5 84.7 77.3 76.8 77.0CasRel𝐿𝑆𝑇𝑀 84.2 83.0 83.6 86.9 80.6 83.7 – – – – – –PMEI𝐿𝑆𝑇𝑀 88.7 86.8 87.8 88.7 87.6 88.1 84.5 84.0 84.2 78.8 77.7 78.2TPLinker𝐿𝑆𝑇𝑀 83.8 83.4 83.6 90.8 90.3 90.5 86.0 82.0 84.0 91.9 81.6 86.4CasRel𝐵𝐸𝑅𝑇 89.7 89.5
8 88.7 87.6 88.1 84.5 84.0 84.2 78.8 77.7 78.2TPLinker𝐿𝑆𝑇𝑀 83.8 83.4 83.6 90.8 90.3 90.5 86.0 82.0 84.0 91.9 81.6 86.4CasRel𝐵𝐸𝑅𝑇 89.7 89.5 89.6 93.4 90.1 91.8 89.8★88.2★89.0★88.3★84.6★86.4★PMEI𝐵𝐸𝑅𝑇 90.5 89.8 90.1 91.0 92.9 92.0 88.4 88.9 88.7 80.8 82.8 81.8TPLinker𝐵𝐸𝑅𝑇 91.
 89.6 93.4 90.1 91.8 89.8★88.2★89.0★88.3★84.6★86.4★PMEI𝐵𝐸𝑅𝑇 90.5 89.8 90.1 91.0 92.9 92.0 88.4 88.9 88.7 80.8 82.8 81.8TPLinker𝐵𝐸𝑅𝑇 91.3 92.5 91.9 91.8 92.0 91.9 91.4 92.6 92.0 88.9 84.5 86.7ConCasRTE 𝐿𝑆𝑇𝑀 88.1 86.6 87.3 91.2 90.8 91.0 86.6 82.3 84.4 88.3 83.9 86.0ConCas
3 92.5 91.9 91.8 92.0 91.9 91.4 92.6 92.0 88.9 84.5 86.7ConCasRTE 𝐿𝑆𝑇𝑀 88.1 86.6 87.3 91.2 90.8 91.0 86.6 82.3 84.4 88.3 83.9 86.0ConCasRTE 𝐵𝐸𝑅𝑇 92.9 92.3 92.6 93.8 92.5 93.1 92.9 92.1 92.5 90.6 88.1 89.3Table 3: F1 scores on sentences with different overlapping pattern and different triplet number. Results of CasRel are copiedfrom TPLinker directly. “T” is the number of triples contained in a sentence.ModelNYT∗WebNLG∗Normal SEO EPO T = 1 T =
RTE 𝐵𝐸𝑅𝑇 92.9 92.3 92.6 93.8 92.5 93.1 92.9 92.1 92.5 90.6 88.1 89.3Table 3: F1 scores on sentences with different overlapping pattern and different triplet number. Results of CasRel are copiedfrom TPLinker directly. “T” is the number of triples contained in a sentence.ModelNYT∗WebNLG∗Normal SEO EPO T = 1 T = 2 T = 3 T = 4 T ≥5 Normal SEO EPO T = 1 T = 2 T = 3 T = 4 T ≥5CasRel𝐵𝐸𝑅𝑇 87.3 91.4 92.0 88.2 90.3 91.9 94.2 83.7 89.4 92.2 94.7 89.3 90.8 94.2 92.4 90.9TPLinker𝐵𝐸𝑅𝑇
 2 T = 3 T = 4 T ≥5 Normal SEO EPO T = 1 T = 2 T = 3 T = 4 T ≥5CasRel𝐵𝐸𝑅𝑇 87.3 91.4 92.0 88.2 90.3 91.9 94.2 83.7 89.4 92.2 94.7 89.3 90.8 94.2 92.4 90.9TPLinker𝐵𝐸𝑅𝑇 90.1 93.4 94.0 90.0 92.8 93.1 96.1 90.0 87.9 92.5 95.3 88.0 90.1 94.6 93.3 91.6ConCasRTE 𝐵𝐸𝑅𝑇 90.6 94.0 94.1 90.5 93.8 93.4 95.2 91
 90.1 93.4 94.0 90.0 92.8 93.1 96.1 90.0 87.9 92.5 95.3 88.0 90.1 94.6 93.3 91.6ConCasRTE 𝐵𝐸𝑅𝑇 90.6 94.0 94.1 90.5 93.8 93.4 95.2 91.7 91.1 93.3 93.8 90.7 91.9 95.5 93.4 91.9Table 4: Detailed Results (F1). ↑means increased scores.Models NYT∗WebNLG∗NYT WebNLGConCasRTE 𝐶𝑒 91.8 91.9 91.6 87.9ConCasRTE 𝐷𝑖𝑓𝑊 92.1 92.4 91.8 88.4ConCas
.7 91.1 93.3 93.8 90.7 91.9 95.5 93.4 91.9Table 4: Detailed Results (F1). ↑means increased scores.Models NYT∗WebNLG∗NYT WebNLGConCasRTE 𝐶𝑒 91.8 91.9 91.6 87.9ConCasRTE 𝐷𝑖𝑓𝑊 92.1 92.4 91.8 88.4ConCasRTE 𝑅𝑒𝑆 92.5 92.7 91.9 88.7ConCasRTE 𝐹𝐿𝑜𝑠 92.5 92.9 92.2 89.0ETL-Span𝐶𝐿𝑜𝑠 78.9(↑0.8) 88.8(↑1.2) 78.8(↑0.8) 84.1(↑1.0)CasRel𝐶𝐿𝑜𝑠 90.0(↑0.4) 9
RTE 𝑅𝑒𝑆 92.5 92.7 91.9 88.7ConCasRTE 𝐹𝐿𝑜𝑠 92.5 92.9 92.2 89.0ETL-Span𝐶𝐿𝑜𝑠 78.9(↑0.8) 88.8(↑1.2) 78.8(↑0.8) 84.1(↑1.0)CasRel𝐶𝐿𝑜𝑠 90.0(↑0.4) 92.3(↑0.5) 89.6(↑0.6) 87.9(↑1.5)TPLinker𝐶𝐿𝑜𝑠 92.2(↑0.3) 92.5(↑0.6) 92.3(↑0.3) 88.1(↑1.4)(i)Ability Evaluation . To evaluate the proposed loss function’sability for addressing the class imbalance issue , we implement fol-lowing variants that use different methods for addressing the men-tioned issue. (1) ConCasRTE �
2.3(↑0.5) 89.6(↑0.6) 87.9(↑1.5)TPLinker𝐶𝐿𝑜𝑠 92.2(↑0.3) 92.5(↑0.6) 92.3(↑0.3) 88.1(↑1.4)(i)Ability Evaluation . To evaluate the proposed loss function’sability for addressing the class imbalance issue , we implement fol-lowing variants that use different methods for addressing the men-tioned issue. (1) ConCasRTE 𝐷𝑖𝑓𝑊 : a variant that assigns differentweights for the losses of positive and negative classes (here 0.75for the positive and 0.25 for the negative). (2) ConCasRTE 𝑅𝑒𝑆: are-sampling based variant that randomly selects some samples fromthe negative class so that makes the proportion between samples inthe classes of positive and negative be a predefined threshold (here is1:5). (3) ConCasRTE 𝐹𝐿𝑜𝑠𝑠 , a variant that uses Focal Loss [11] (its hy-perparameter 𝛾is set to
�𝑖𝑓𝑊 : a variant that assigns differentweights for the losses of positive and negative classes (here 0.75for the positive and 0.25 for the negative). (2) ConCasRTE 𝑅𝑒𝑆: are-sampling based variant that randomly selects some samples fromthe negative class so that makes the proportion between samples inthe classes of positive and negative be a predefined threshold (here is1:5). (3) ConCasRTE 𝐹𝐿𝑜𝑠𝑠 , a variant that uses Focal Loss [11] (its hy-perparameter 𝛾is set to 2). We can see that the proposed loss bringsthe greatest performance improvement over ConCasRTE 𝐶𝑒thanall the compared methods, which demonstrates the proposed lossis more effective. Different from existing state-of-the-art methodslikeFocal Loss , the proposed loss function does not try to increasethe importance of samples in the minor classes. Instead, it directlyremoves some samples in the major classes so as to narrow thenumber gap between samples in the major and minor classes. Thesecomparison results show this strategy is more effective.(ii)Adaptability Evaluation . The proposed loss is applicable toa wide range
 2). We can see that the proposed loss bringsthe greatest performance improvement over ConCasRTE 𝐶𝑒thanall the compared methods, which demonstrates the proposed lossis more effective. Different from existing state-of-the-art methodslikeFocal Loss , the proposed loss function does not try to increasethe importance of samples in the minor classes. Instead, it directlyremoves some samples in the major classes so as to narrow thenumber gap between samples in the major and minor classes. Thesecomparison results show this strategy is more effective.(ii)Adaptability Evaluation . The proposed loss is applicable toa wide range of models since we don’t make any assumptionsabout the data distribution. For example, it can be used not only inthe tagging based methods, but also in other kinds of methods. Toevaluate this, we transplant it to following diverse models includingETL-Span ,CasRel , and TPLinker . These new models are marked bya subscript “ CLos ”. Results show that all these new models achievesignificant improvement over their original ones on all datasets.4 CONCLUSIONSIn this paper, we propose two novelties to address the class imbal-ance issue existed in the tagging based RTE methods, which are ath
 of models since we don’t make any assumptionsabout the data distribution. For example, it can be used not only inthe tagging based methods, but also in other kinds of methods. Toevaluate this, we transplant it to following diverse models includingETL-Span ,CasRel , and TPLinker . These new models are marked bya subscript “ CLos ”. Results show that all these new models achievesignificant improvement over their original ones on all datasets.4 CONCLUSIONSIn this paper, we propose two novelties to address the class imbal-ance issue existed in the tagging based RTE methods, which are athree-step extraction framework and a confidence threshold basedcross entropy loss function. To the best of our knowledge, this is thefirst work to explore this issue in RTE. We evaluate the proposedmodel on two benchmark datasets. Experiments show that bothnovelties can alleviate the class imbalance issue effectively, and theyhelp our model achieve state-of-the-art results on both datasets.ACKNOWLEDGMENTSThis work is supported by the National Key R&D Program of China(No.2018YFC0830701), the National Natural Science Foundationof China (No.61572120),
ree-step extraction framework and a confidence threshold basedcross entropy loss function. To the best of our knowledge, this is thefirst work to explore this issue in RTE. We evaluate the proposedmodel on two benchmark datasets. Experiments show that bothnovelties can alleviate the class imbalance issue effectively, and theyhelp our model achieve state-of-the-art results on both datasets.ACKNOWLEDGMENTSThis work is supported by the National Key R&D Program of China(No.2018YFC0830701), the National Natural Science Foundationof China (No.61572120), the Fundamental Research Funds for theCentral Universities (No.N181602013 and No.N171602003). REFERENCES[1]Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018.Joint entity recognition and relation extraction as a multi-head selection problem.Expert Systems With Applications 114 (2018), 34–45.[2]Yee Seng Chan and Dan Roth. 2011. Exploiting Syntactico-Semantic Structures forRelation
 the Fundamental Research Funds for theCentral Universities (No.N181602013 and No.N171602003). REFERENCES[1]Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018.Joint entity recognition and relation extraction as a multi-head selection problem.Expert Systems With Applications 114 (2018), 34–45.[2]Yee Seng Chan and Dan Roth. 2011. Exploiting Syntactico-Semantic Structures forRelation Extraction. In Proceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human Language Technologies . 551–560.[3] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. 2019. Class-Balanced Loss Based on Effective Number of Samples. In 2019 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR) . 9268–9277.[4]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina N
 Extraction. In Proceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human Language Technologies . 551–560.[3] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. 2019. Class-Balanced Loss Based on Effective Number of Samples. In 2019 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR) . 9268–9277.[4]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina N. Toutanova. 2018.BERT: Pre-training of Deep Bidirectional Transformers for Language Under-standing. In Proceedings of the 2019 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies,Volume 1 (Long and Short Papers) . 4171–4186.[5]Markus Eberts and Adrian Ulges. 2019. Span-Based Joint Entity and RelationExtraction with Transformer Pre-Training.. In ECAI . 2006–2013.[6]Tsu-Jui Fu,
. Toutanova. 2018.BERT: Pre-training of Deep Bidirectional Transformers for Language Under-standing. In Proceedings of the 2019 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies,Volume 1 (Long and Short Papers) . 4171–4186.[5]Markus Eberts and Adrian Ulges. 2019. Span-Based Joint Entity and RelationExtraction with Transformer Pre-Training.. In ECAI . 2006–2013.[6]Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma. 2019. GraphRel: Modeling Text asRelational Graphs for Joint Entity and Relation Extraction. In Proceedings of the57th Annual Meeting of the Association for Computational Linguistics . 1409–1418.[7]Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating Training Corpora for NLG Micro-Planners. In Proceed-ings of the 55th Annual Meeting of the Association for Computational Linguistics(V
 Peng-Hsuan Li, and Wei-Yun Ma. 2019. GraphRel: Modeling Text asRelational Graphs for Joint Entity and Relation Extraction. In Proceedings of the57th Annual Meeting of the Association for Computational Linguistics . 1409–1418.[7]Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating Training Corpora for NLG Micro-Planners. In Proceed-ings of the 55th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers) . 179–188.[8]Pankaj Gupta, Hinrich Schütze, and Bernt Andrassy. 2016. Table Filling Multi-Task Recurrent Neural Network for Joint Entity and Relation Extraction. InProceedings of COLING 2016, the 26th International Conference on ComputationalLinguistics: Technical Papers . The COLING 2016 Organizing Committee, Osaka,Japan, 2537–2547.[9]Justin M. Johnson and Taghi M. Khoshgo
olume 1: Long Papers) . 179–188.[8]Pankaj Gupta, Hinrich Schütze, and Bernt Andrassy. 2016. Table Filling Multi-Task Recurrent Neural Network for Joint Entity and Relation Extraction. InProceedings of COLING 2016, the 26th International Conference on ComputationalLinguistics: Technical Papers . The COLING 2016 Organizing Committee, Osaka,Japan, 2537–2547.[9]Justin M. Johnson and Taghi M. Khoshgoftaar. 2019. Survey on deep learningwith class imbalance. Journal of Big Data 6, 1 (2019), 1–54.[10] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-mization. In 3rd International Conference on Learning Representations, ICLR 2015,San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , Yoshua Bengioand Yann LeCun (Eds.).[1
ftaar. 2019. Survey on deep learningwith class imbalance. Journal of Big Data 6, 1 (2019), 1–54.[10] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-mization. In 3rd International Conference on Learning Representations, ICLR 2015,San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , Yoshua Bengioand Yann LeCun (Eds.).[11] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. 2020.Focal Loss for Dense Object Detection. IEEE Transactions on Pattern Analysis andMachine Intelligence 42, 2 (2020), 318–327.[12] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain,Andreas Veit, and Sanjiv Kumar. 2020. Long-tail learning via logit adjustment.arXiv
1] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. 2020.Focal Loss for Dense Object Detection. IEEE Transactions on Pattern Analysis andMachine Intelligence 42, 2 (2020), 318–327.[12] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain,Andreas Veit, and Sanjiv Kumar. 2020. Long-tail learning via logit adjustment.arXiv preprint arXiv:2007.07314 (2020).[13] Makoto Miwa and Mohit Bansal. 2016. End-to-End Relation Extraction usingLSTMs on Sequences and Tree Structures. In Proceedings of the 54th AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) .Association for Computational Linguistics, Berlin, Germany, 1105–1116.[14] Tapas Nayak and Hwee Tou Ng. 2020. Effective Modeling of
 preprint arXiv:2007.07314 (2020).[13] Makoto Miwa and Mohit Bansal. 2016. End-to-End Relation Extraction usingLSTMs on Sequences and Tree Structures. In Proceedings of the 54th AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) .Association for Computational Linguistics, Berlin, Germany, 1105–1116.[14] Tapas Nayak and Hwee Tou Ng. 2020. Effective Modeling of Encoder-DecoderArchitecture for Joint Entity and Relation Extraction. Proceedings of the AAAIConference on Artificial Intelligence 34, 5 (2020), 8528–8535.[15] Tapas Nayak and Hwee Tou Ng. 2020. Effective Modeling of Encoder-DecoderArchitecture for Joint Entity and Relation Extraction. In The Thirty-Fourth AAAIConference on Artificial Intelligence, AAAI 2020, The Thirty-Second InnovativeApplications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Sym-posium on Educational
 Encoder-DecoderArchitecture for Joint Entity and Relation Extraction. Proceedings of the AAAIConference on Artificial Intelligence 34, 5 (2020), 8528–8535.[15] Tapas Nayak and Hwee Tou Ng. 2020. Effective Modeling of Encoder-DecoderArchitecture for Joint Entity and Relation Extraction. In The Thirty-Fourth AAAIConference on Artificial Intelligence, AAAI 2020, The Thirty-Second InnovativeApplications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Sym-posium on Educational Advances in Artificial Intelligence, EAAI 2020, New York,NY, USA, February 7-12, 2020 . AAAI Press, 8528–8535.[16] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:Global vectors for word representation. In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing (EMNLP) . 1532–1543.[17] Sebastian Riedel, Limin Yao, and
 Advances in Artificial Intelligence, EAAI 2020, New York,NY, USA, February 7-12, 2020 . AAAI Press, 8528–8535.[16] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:Global vectors for word representation. In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing (EMNLP) . 1532–1543.[17] Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relationsand their mentions without labeled text. In Joint European Conference on MachineLearning and Knowledge Discovery in Databases . 148–163.[18] Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong Liu. 2020.Recurrent Interaction Network for Jointly Extracting Entities and ClassifyingRelations. In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2020, Online, November 16-20, 20
 Andrew McCallum. 2010. Modeling relationsand their mentions without labeled text. In Joint European Conference on MachineLearning and Knowledge Discovery in Databases . 148–163.[18] Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong Liu. 2020.Recurrent Interaction Network for Jointly Extracting Entities and ClassifyingRelations. In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2020, Online, November 16-20, 2020 , Bonnie Webber,Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for ComputationalLinguistics, 3722–3732.[19] Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong Liu. 2021.Progressive Multitask Learning with Controlled Information Flow for Joint En-tity and Relation Extraction. In Association for the Advancement of ArtificialIntelligence (AAAI) .[20] Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu, Hongsong Zhu
20 , Bonnie Webber,Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for ComputationalLinguistics, 3722–3732.[19] Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong Liu. 2021.Progressive Multitask Learning with Controlled Information Flow for Joint En-tity and Relation Extraction. In Association for the Advancement of ArtificialIntelligence (AAAI) .[20] Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu, Hongsong Zhu, andLimin Sun. 2020. TPLinker: Single-stage Joint Extraction of Entities and RelationsThrough Token Pair Linking. In Proceedings of the 28th International Conferenceon Computational Linguistics . Barcelona, Spain (Online), 1572–1582.[21] Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang. 2020. A NovelCascade Binary Tagging Framework for Relational Triple Extraction. In Proceed-ings of the 58th Annual Meeting of the Association for Computational Linguistics .Association for Computational Linguistics
, andLimin Sun. 2020. TPLinker: Single-stage Joint Extraction of Entities and RelationsThrough Token Pair Linking. In Proceedings of the 28th International Conferenceon Computational Linguistics . Barcelona, Spain (Online), 1572–1582.[21] Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang. 2020. A NovelCascade Binary Tagging Framework for Relational Triple Extraction. In Proceed-ings of the 58th Annual Meeting of the Association for Computational Linguistics .Association for Computational Linguistics, Online, 1476–1488.[22] Bowen Yu, Zhenyu Zhang, Xiaobo Shu, Tingwen Liu, Yubin Wang, Bin Wang,and Sujian Li. 2019. Joint Extraction of Entities and Relations Based on a NovelDecomposition Strategy.. In ECAI . 2282–2289.[23] Yue Yuan, Xiaofei Zhou, Shirui Pan, Qiannan Zhu, Zeliang Song, and Li Guo.2020. A relation-specific attention network for joint entity and
, Online, 1476–1488.[22] Bowen Yu, Zhenyu Zhang, Xiaobo Shu, Tingwen Liu, Yubin Wang, Bin Wang,and Sujian Li. 2019. Joint Extraction of Entities and Relations Based on a NovelDecomposition Strategy.. In ECAI . 2282–2289.[23] Yue Yuan, Xiaofei Zhou, Shirui Pan, Qiannan Zhu, Zeliang Song, and Li Guo.2020. A relation-specific attention network for joint entity and relation extraction.InProceedings of the Twenty-Ninth International Joint Conference on ArtificialIntelligence , Vol. 4. 4054–4060.[24] Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methodsfor relation extraction. Journal of Machine Learning Research 3, 6 (2003), 1083–1106.[25] Daojian Zeng, Haoran Zhang, and Qianying Liu. 2020. CopyMTL: Copy Mech-anism for Joint Extraction of Entities
 relation extraction.InProceedings of the Twenty-Ninth International Joint Conference on ArtificialIntelligence , Vol. 4. 4054–4060.[24] Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methodsfor relation extraction. Journal of Machine Learning Research 3, 6 (2003), 1083–1106.[25] Daojian Zeng, Haoran Zhang, and Qianying Liu. 2020. CopyMTL: Copy Mech-anism for Joint Extraction of Entities and Relations with Multi-Task Learning.Proceedings of the AAAI Conference on Artificial Intelligence 34, 5 (2020), 9507–9514.[26] Xiangrong Zeng, Shizhu He, Daojian Zeng, Kang Liu, Shengping Liu, and JunZhao. 2019. Learning the Extraction Order of Multiple Relational Facts in aSentence with Reinforcement Learning. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processing and the 9th International JointConference on Natural Language Processing (EM
 and Relations with Multi-Task Learning.Proceedings of the AAAI Conference on Artificial Intelligence 34, 5 (2020), 9507–9514.[26] Xiangrong Zeng, Shizhu He, Daojian Zeng, Kang Liu, Shengping Liu, and JunZhao. 2019. Learning the Extraction Order of Multiple Relational Facts in aSentence with Reinforcement Learning. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processing and the 9th International JointConference on Natural Language Processing (EMNLP-IJCNLP) . 367–377.[27] Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu, and Jun Zhao. 2018. Ex-tracting Relational Facts by an End-to-End Neural Model with Copy Mechanism.InProceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , Vol. 1. 506–514.[28] Meishan Zhang, Yue Zhang, and Guohong Fu. 
NLP-IJCNLP) . 367–377.[27] Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu, and Jun Zhao. 2018. Ex-tracting Relational Facts by an End-to-End Neural Model with Copy Mechanism.InProceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , Vol. 1. 506–514.[28] Meishan Zhang, Yue Zhang, and Guohong Fu. 2017. End-to-End Neural RelationExtraction with Global Optimization. In Proceedings of the 2017 Conference onEmpirical Methods in Natural Language Processing . Association for ComputationalLinguistics, Copenhagen, Denmark, 1730–1740.[29] Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu.2017. Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme.InProceedings of the 55th Annual Meeting of the Association for ComputationalLinguistics (Volume 1:
2017. End-to-End Neural RelationExtraction with Global Optimization. In Proceedings of the 2017 Conference onEmpirical Methods in Natural Language Processing . Association for ComputationalLinguistics, Copenhagen, Denmark, 1730–1740.[29] Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu.2017. Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme.InProceedings of the 55th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers) , Vol. 1. 1227–1236.[30] GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring VariousKnowledge in Relation Extraction. In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics (ACL’05) . 427–434.[31] Yang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, and Jinsong Wang. 2018. Unsuper-vised Domain Adaptation for
 Long Papers) , Vol. 1. 1227–1236.[30] GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring VariousKnowledge in Relation Extraction. In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics (ACL’05) . 427–434.[31] Yang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, and Jinsong Wang. 2018. Unsuper-vised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-training. In Proceedings of the European Conference on Computer Vision (ECCV) .297–313.
 Semantic Segmentation via Class-Balanced Self-training. In Proceedings of the European Conference on Computer Vision (ECCV) .297–313.
